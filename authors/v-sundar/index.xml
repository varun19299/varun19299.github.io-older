<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>V Sundar | Varun Sundar</title>
    <link>https://varun19299.github.io/authors/v-sundar/</link>
      <atom:link href="https://varun19299.github.io/authors/v-sundar/index.xml" rel="self" type="application/rss+xml" />
    <description>V Sundar</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 11 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://varun19299.github.io/img/icon-192.png</url>
      <title>V Sundar</title>
      <link>https://varun19299.github.io/authors/v-sundar/</link>
    </image>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references-cmu/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references-cmu/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Varun Sundar&lt;/strong&gt;, Salman Siddique and Kaushik Mitra, &lt;strong&gt;“A Unified Framework for Lensless Image Recovery”&lt;/strong&gt;, to be submitted to the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;H Drew and M Christopher. GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Varun Sundar&lt;/strong&gt;, Arka Sadhu and Ram Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chia-Yin Tsai, Aswin C. Sankaranarayanan, and Ioannis Gkioulekas. &amp;ldquo;Beyond Volumetric Albedo—A Surface Optimization Framework for Non-Line-of-Sight Imaging&amp;rdquo;, CVPR 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Shumian Xin, Sotiris Nousias, Kiriakos N. Kutulakos, Aswin C. Sankaranarayanan, Srinivasa G. Narasimhan, and Ioannis Gkioulekas. &amp;ldquo;A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction&amp;rdquo;, CVPR 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;David B. Lindell, Matthew O’Toole, and Gordon Wetzstein. 2018. Single-Photon 3D Imaging with Deep  Sensor Fusion. ACM Trans. Graph. 37, 4, Article 113 (August 2018), 12 pages.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references-princeton/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references-princeton/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Varun Sundar&lt;/strong&gt;, Salman Siddique and Kaushik Mitra, &lt;strong&gt;“A Unified Framework for Lensless Image Recovery”&lt;/strong&gt;, to be submitted in the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;H Drew and M Christopher. GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Varun Sundar&lt;/strong&gt;, Arka Sadhu and Ram Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tobias Gruber, Frank D. Julca-Aguilar, Mario Bijelic, Werner Ritter, Klaus Dietmayer, and Felix Heide. Gated2depth: Real-time dense lidar from gated images. CoRR, abs/1902.04997, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 5. [Attribute Transfer Blog Post](/research/attribute-transfer).
   
1. [MRAS Bandits Blog Post](/research/mras). --&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references-ucla/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references-ucla/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;H Drew and M Christopher. GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia. &lt;strong&gt;FARCNN: Decoupling attributes from object detection&lt;/strong&gt;. In Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra. &lt;strong&gt;A Unified Framework for Lensless Image Recovery”&lt;/strong&gt;. To be submitted in the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A Kadambi, V Taamazyan, B Shi, and R Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A Kadambi and R Raskar. Rethinking Machine Vision Time of Flight With GHz Heterodyning. In IEEE Access, 2017.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references-uw-madison/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references-uw-madison/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra, &lt;strong&gt;“A Unified Framework for Lensless Image Recovery&lt;/strong&gt;, to be submitted to the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Anant Gupta, Atul Ingle and Mohit Gupta. Asynchronous Single-Photon 3D Imaging. In International Conference on Computer Vision (ICCV), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Felipe Gutierrez, SA Reza, A Velten, Mohit Gupta. Practical Coding Function Design for Time-of-Flight Imaging. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Extending LSD SLAM with Computational Photography</title>
      <link>https://varun19299.github.io/post/extend-lsd-slam/</link>
      <pubDate>Thu, 28 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/post/extend-lsd-slam/</guid>
      <description>

&lt;p&gt;In this post, we shall explore the key steps of &lt;strong&gt;Large Scale Direct SLAM&lt;/strong&gt; (LSD SLAM) and understand its main strengths and weaknesses. We shall also see how we can possibly overcome some of these limitations with recent advances in Computational Photography.&lt;/p&gt;

&lt;h2 id=&#34;what-is-lsd-slam&#34;&gt;What is LSD SLAM?&lt;/h2&gt;

&lt;p&gt;LSD SLAM stands for &lt;strong&gt;Large Scale Direct SLAM&lt;/strong&gt;, implying that it performs simultaneous localisation and mapping &lt;em&gt;directly&lt;/em&gt; on the entire set of image intensities, instead of relying on a subset of points such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Scale-invariant_feature_transform&#34; target=&#34;_blank&#34;&gt;SIFT&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test&#34; target=&#34;_blank&#34;&gt;FAST&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Oriented_FAST_and_rotated_BRIEF&#34; target=&#34;_blank&#34;&gt;ORB&lt;/a&gt; features. Essentially, this allows LSD SLAM to utilise all the information available in the image and thereby not discard crucial information contained in edges, which constitutes a major portion of the scene being mapped.&lt;/p&gt;

&lt;p&gt;Where LSD SLAM crucially differs from earlier direct SLAM methods \cite, is the ingenious methods adopted for tracking and depth estimation. Depth estimation is done via &lt;strong&gt;small baseline&lt;/strong&gt; stereo which allows for much fewer outliers. Tracking is done using direct image alignment over a subset of frames. These modifications allow LSD SLAM to be run real-time even with just a CPU. In comparison, previous approaches such as \cite required a complete high-end GPU to run real-time. &lt;strong&gt;The main novelty in this work include:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;utilising key-frames to simplify monocular SLAM. Semi-dense depth maps are propogated only for key-frames and map optimisation is done only for their poses. &lt;strong&gt;&lt;em&gt;This reduces the complexity of the bundle adjustment problem by a large margin.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;utilising a scale-aware image alignment algorithm to directly estimate the similarity transform &lt;code&gt;$\xi ∈ \mathcal{sim}(3)$&lt;/code&gt; between two keyframes. &lt;strong&gt;&lt;em&gt;By doing so, we aren&amp;rsquo;t restricted to only points pertaining to hand-engineered features.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Incorporation of uncertainty of the estimated depth into tracking. For every point, before solving the correspondence problem, we roughly estimate whether it is worthwhile to do so, ie., &lt;strong&gt;&lt;em&gt;is the computational cost worth the gain in depth accuracy?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, &lt;code&gt;$\mathcal{sim}-3$&lt;/code&gt; is the lie algebra of the Special Euclidean Objects group (SE-3). A lie algebra offers easier optimisation of variables by linearising the manifold of consideration. Intuitively, if we consider just rotation as a matrix, then it must be constrained to be orthogonal &lt;code&gt;($U^TU=I$)&lt;/code&gt;. However, if we were optimising it with, say, gradient descent, then there is no guarantee that the resulting matrix would be orthogonal. Further, we only want to optimise on a subset of &lt;code&gt;$\mathbb{R}^{4\times 4}$&lt;/code&gt;. Lie algebras encapsulate this effectively: the resultant &lt;code&gt;$\mathcal{sim}-3 \in \mathbb{R}^6$&lt;/code&gt; can be optimised by gradient descent. For more details, we refer you to (2).&lt;/p&gt;

&lt;h2 id=&#34;main-pipeline&#34;&gt;Main Pipeline&lt;/h2&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;pipeline.png&#34; data-caption=&#34;Fig 1: Overview of LSD SLAM.&#34;&gt;
&lt;img src=&#34;pipeline.png&#34; alt=&#34;&#34; width=&#34;1400&#34; height=&#34;1200&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 1: Overview of LSD SLAM.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;strong&gt;The pipeline follows:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Compute incoming pose via direct image alignment&lt;/li&gt;
&lt;li&gt;Decide whether to initialise a key-frame&lt;/li&gt;
&lt;li&gt;Compute depth via small baseline stereo&lt;/li&gt;
&lt;li&gt;Repeat, finally perform map optimisation to correct for scale ambiguity&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We shall elaborate on each of these main steps in the sections below.&lt;/p&gt;

&lt;h3 id=&#34;concept-of-keyframes&#34;&gt;Concept of Keyframes&lt;/h3&gt;

&lt;p&gt;As mentioned before, LSD SLAM heavily utilises the concept of keyframes: &amp;ldquo;special&amp;rdquo; frames, which are initialised when a new frame has a pose significantly different from the current keyframe. The very first frame is taken to be a key-frame, with random normal depth, and random normal depth uncertainity. Map optimisation is performed only on the set of key-frames, further lowering computational costs associated with bundle adjustment and loop closure.&lt;/p&gt;

&lt;p&gt;Each key-frame $K_j$ consists of an &lt;strong&gt;image frame, depth map, uncertainity map and pose vector&lt;/strong&gt; - denoted by the tuple &lt;code&gt;$(I_j, D_j, U_j, p_j)$&lt;/code&gt;, where &lt;code&gt;$I_j \in \mathbb{R}^{m\times n \times 3}$&lt;/code&gt;, &lt;code&gt;$D_j \in \mathbb{R^+}^{m\times n}$&lt;/code&gt;, &lt;code&gt;$U_j \in \mathbb{R^+}^{m\times n}$&lt;/code&gt; and &lt;code&gt;$p_j \in \mathbb{R}^6$&lt;/code&gt;. For non-key frames, we are not concerned with depth or uncertainity, and each non-key frame &lt;code&gt;$NK_j$&lt;/code&gt; consists of the tuple &lt;code&gt;$(I_j, p_j)$&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;direct-image-alignment&#34;&gt;Direct Image Alignment&lt;/h3&gt;

&lt;p&gt;The objective of direct image aligment is to use a key-frame  &lt;code&gt;$K_i$&lt;/code&gt; and a new-frame &lt;code&gt;$I_j$&lt;/code&gt; to determine &lt;code&gt;$p_j$&lt;/code&gt;. This is done by a warping, where we project points from one image using the available depth information. If we know the pose of the second image, then we can recover the second image from these projected points. Thus, by optimising a function of the form below, we can find the pose of the second image.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$\begin{align} p_j = \text{argmin}_p f(I_i - I_j[warp(D_i, p)]) \end{align} $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Commonly used &lt;code&gt;$f$&lt;/code&gt;, also called residue function, include &lt;code&gt;$f(x - y) = ||x-y||^2$&lt;/code&gt; (MSE), or the hinge function (which ignores outliers), or L1 distance etc.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;warping.gif&#34; data-caption=&#34;Fig 2: Warping images from the TUM room sequence.&#34;&gt;
&lt;img src=&#34;warping.gif&#34; alt=&#34;&#34; width=&#34;1200&#34; height=&#34;750&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 2: Warping images from the &lt;a href=&#34;https://vision.in.tum.de/data/datasets/rgbd-dataset/download&#34; target=&#34;_blank&#34;&gt;TUM room sequence&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;In the above figure, we demonstrate inverse image warping. These images have been taken from the &lt;a href=&#34;https://vision.in.tum.de/data/datasets/rgbd-dataset/download&#34; target=&#34;_blank&#34;&gt;TUM room sequence&lt;/a&gt;, a common SLAM benchmarking dataset. Black regions correspond to points where the Kinect failed to output any depth. Except when the target pose matches the source pose, this results in zero intensity regions. Providing a more complete depth map can offset this.&lt;/p&gt;

&lt;p&gt;Since warping is so sensitive to depth information, it is important we optimise this function or residue only over points with lower variance in depth. This is where we can incorporate the uncertainity matrix &lt;code&gt;$U_i$&lt;/code&gt;. One way of doing this is to divide the residue function element-wise by the uncertainity, yielding:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$\begin{align} p_j = \text{argmin}_p f(\frac{I_i - I_j[warp(D_i, p)]}{g(U_i)}) \end{align} $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The warp function, &lt;code&gt;$w(D, p):$ pixel $\to$ pixel&lt;/code&gt;, takes a depth map &lt;code&gt;$D$&lt;/code&gt; and a pixel point &lt;code&gt;$p$&lt;/code&gt; and computes the corresponding pixel point in frame &lt;code&gt;$j$&lt;/code&gt;. This requires projection to 3D world coordinates and re-projection to the second camera frame. Both of these require knowledge of the &lt;a href=&#34;http://ftp.cs.toronto.edu/pub/psala/VM/camera-parameters.pdf&#34; target=&#34;_blank&#34;&gt;camera parameters&lt;/a&gt;, which we implicitly assume to be known here. Written as a minimisation objective (sum over all pixels &lt;code&gt;$q \in \Sigma_D \subset [m]\times[n]$&lt;/code&gt; in the image), this becomes:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$\begin{align} p_j = \text{argmin}_p \sum_{q \in \Sigma_D \subset [m]\times[n]} f(\frac{I_i[q] - I_j[warp(D_i[q], p)]}{g(U_i[q])}) \end{align} $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Giving us a similar objective as in Figure 1. LSD SLAM adopts &lt;code&gt;$g(U_i[q])= 2\sigma^2 + \frac{df(I_i[q] - I_j[warp(D_i[q], p)])}{dD_i[q]}^2U_i[q]$&lt;/code&gt;. Intuitively, this penalises regions with continuous image intensity variation lesser than those with abrupt variations. We minimise this objective only over a subset of points for which we know depth information, hence being &lt;strong&gt;semi-dense image alignment&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Also note that we have not utilised quite a bit of prior information here. For instance, in the above picture, if you knew two corners of the monitor, you know where the other lie, since the monitor is rectangular. Using such information (priors) can greatly help improve the optimisation landscape. We shall discuss more about incorporating such data-driven priors in the limitations section.&lt;/p&gt;

&lt;p&gt;Finally, we are optimising over the pose vector &lt;code&gt;$p_j$&lt;/code&gt;. As a matrix, which is commonly used for such projective functions, the pose matrix has 16 entries. However, due to scale and rigid body constraints, this works out to just 6 degrees of freedom (dof). It is important to optimise the function over a linear landscape (or manifold), since it makes it much easier to converge towards an optimal solution. Here&amp;rsquo;s where the &lt;strong&gt;lie algebra&lt;/strong&gt; comes into play: it represents a linear manifold of the lie group, which is our pose matrix here. You can employ familiar first and second order optimisers including gradient descent, newton&amp;rsquo;s method etc., but the most common is &lt;a href=&#34;https://en.wikipedia.org/wiki/Gauss–Newton_algorithm&#34; target=&#34;_blank&#34;&gt;Gauss-Newton&lt;/a&gt; optimisation.&lt;/p&gt;

&lt;h3 id=&#34;choosing-key-frames&#34;&gt;Choosing Key-frames&lt;/h3&gt;

&lt;p&gt;The motivation for choosing key-frames follows a very simple observation - LSD SLAM utilises small-baseline stereo matching, and hence it would be desirable if all the frames belonging to a particular key-frame group (all frames &lt;code&gt;$NK_j$&lt;/code&gt; between &lt;code&gt;$K_i, K_{i+1}$&lt;/code&gt;) are &amp;ldquo;close&amp;rdquo; to the key-frame &lt;code&gt;$K_i$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Hence, initialise a new key-frame if,&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$||p_j - p_i||&amp;gt; \rho $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;When you initialise a new key-frame, use the projected points from the previous key-frames to initialise its depth map. The uncertainity map is similarly projected. Note that this might be semi-dense since not all projected points would cover the new key-frame. Here, to incorporate scale-drift tracking, LSD SLAM normalises the mean inverse depth to be 1. Scale awareness is incorporated during the final map-optimisation across various key-frames.&lt;/p&gt;

&lt;h3 id=&#34;small-baseline-stereo-matching&#34;&gt;Small baseline stereo matching&lt;/h3&gt;

&lt;p&gt;We perform many small baseline stereo-matching operations to refine the depth map of the key-frame &lt;code&gt;$K_i$&lt;/code&gt;. We do not compute depth maps for non-key frames. In a similar manner to (2), only refinements where stereo depth is of higher confidence based on the accuracy of disparity search. This essentially allows us to propogate a semi-dense depth map throughout the whole SLAM pipeline, aiding computational simplicity. Such regions are often those with rich texture, which is expected in stereo based depth estimation.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;lsd_depth.jpeg&#34; data-caption=&#34;Fig 3: Small-baseline stereo matching for depth refinement. (Left) Input frame, (Right) Semi-dense depth estimates.&#34;&gt;
&lt;img src=&#34;lsd_depth.jpeg&#34; alt=&#34;&#34; width=&#34;900&#34; height=&#34;400&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 3: Small-baseline stereo matching for depth refinement. (&lt;em&gt;Left&lt;/em&gt;) Input frame, (&lt;em&gt;Right&lt;/em&gt;) Semi-dense depth estimates.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;map-optimisation&#34;&gt;Map Optimisation&lt;/h3&gt;

&lt;p&gt;At the end of tracking, we obtain a set of key-frames and their coarse pose-estimates. We are left with two pending tasks: (a) &lt;strong&gt;Incorporate global scale into these coarse pose-estimates&lt;/strong&gt;. Remember, we initialised the depth map of a new key-frame with mean inverse depth as 1. (b) &lt;strong&gt;Further refine poses by minimising overall entropy&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We achieve the first by doing a similar photometric residual minimisation, but here over &lt;code&gt;$\xi ∈ \mathcal{sim}(3)$&lt;/code&gt;, which has 7-degrees of freedom, the additional dof representing scale. This allows us to find scale parameters which best satisfy other key-frame constraints. This is written as:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$\begin{align} p_j = \text{argmin}_p \sum_{q \in \Sigma_D \subset [m]\times[n]} f(\frac{I_i[q] - I_j[warp(D_i[q], p)]}{g_1(U_i[q])} + \frac{D_i[q] - D_j[warp(D_i[q], p)]}{g_2(U_i[q])}) \end{align} $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here, &lt;code&gt;$g_2$&lt;/code&gt; is another smoothening term with respect to &lt;code&gt;$D_i[q] - D_j[warp(K_i[q], p)]$&lt;/code&gt;. Again observe that map-optimisation is performed only among key-frames, hence we have access to &lt;code&gt;$D_j$&lt;/code&gt;. In practice, &lt;code&gt;$\mathcal{sim}(3)$&lt;/code&gt; tracking is computationally similar to &lt;code&gt;$\mathcal{se}(3)$&lt;/code&gt; tracking.&lt;/p&gt;

&lt;h2 id=&#34;limitations-of-lsd-slam&#34;&gt;Limitations of LSD SLAM&lt;/h2&gt;

&lt;p&gt;While LSD SLAM certainly does produce quite impressive results, it does have its fair share of limitations too. We discuss the fundamental challenges behind a few of these below. Most of these limitations arise due to the restricted information available in a monocular video stream. And this is precisely where Computational Photography can be leveraged. By further considering information from physical camera models and their corresponding optics, we can alleviate a few of these challenges.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;lsd_slam_run.gif&#34; data-caption=&#34;Fig 4: LSD SLAM results.&#34;&gt;
&lt;img src=&#34;lsd_slam_run.gif&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 4: LSD SLAM results.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;scale-ambiguity&#34;&gt;Scale Ambiguity&lt;/h3&gt;

&lt;p&gt;Monocular depth estimation techniques tends to lack a sense of absolute global scale. Thus, although our relative pose estimations within each key-frame may be accurate, this is no guarantee regarding the global map. While the map-optimisation procedure aims to mitigate this ambiguity, it is worthwhile noting that the &lt;code&gt;$\mathcal{sim}(3)$&lt;/code&gt; objective is non-convex and hence depends to a major extent on the initialisation parameters. This step limits the convergence radius, which is how far LSD SLAM works before producing an inconsistent map.&lt;/p&gt;

&lt;p&gt;A later paper from TU Munich - &amp;ldquo;CNN SLAM&amp;rdquo;, CVPR 2017 (4) -&lt;/p&gt;

&lt;p&gt;Why CNN SLAM doesn&amp;rsquo;t cut it.&lt;/p&gt;

&lt;h3 id=&#34;depth-from-defocus-coded-aperture&#34;&gt;Depth from Defocus: Coded Aperture&lt;/h3&gt;

&lt;h3 id=&#34;pure-rotation-settings&#34;&gt;Pure Rotation Settings&lt;/h3&gt;

&lt;h2 id=&#34;other-extensions&#34;&gt;Other Extensions&lt;/h2&gt;

&lt;h3 id=&#34;using-polarisation-for-texture-less-regions&#34;&gt;Using polarisation for texture-less regions&lt;/h3&gt;

&lt;h3 id=&#34;explicit-priors-for-pose-estimation&#34;&gt;Explicit priors for pose-estimation&lt;/h3&gt;

&lt;p&gt;Make pose-estimation a lot cheaper.&lt;/p&gt;

&lt;h3 id=&#34;can-we-better-represent-maps&#34;&gt;Can we better represent maps?&lt;/h3&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Thomas Schöps, Jacob Engel and Daniel Cremers. &lt;a href=&#34;https://vision.in.tum.de/research/vslam/lsdslam&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;LSD-SLAM: Large-Scale Direct Monocular SLAM&amp;rdquo;&lt;/a&gt;. In Proceedings of the European Conference on Computer Vision (ECCV), 2014.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Jose Blanco.&lt;a href=&#34;http://ingmec.ual.es/~jlblanco/papers/jlblanco2010geometry3D_techrep.pdf&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;A tutorial on SE(3) transformation parameterizations and on-manifold optimization&amp;rdquo;&lt;/a&gt;. MAPIR Group.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Jacob Engel, Jurgen Sturm and Daniel Cremers. &lt;a href=&#34;https://jsturm.de/publications/data/engel2013iccv.pdf&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Semi-Dense Visual Odometry for a Monocular Camera&amp;rdquo;&lt;/a&gt;. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Keisuke Tatento, Federico Tombari, Iro Laina and Nassir Navab. &lt;a href=&#34;https://arxiv.org/abs/1704.03489&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction&amp;rdquo;&lt;/a&gt;. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2017.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Attribute Transfer in a GAN Framework</title>
      <link>https://varun19299.github.io/research/attribute-transfer/</link>
      <pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/research/attribute-transfer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Presentations in CS7020: Advances in the Theory of Deep Learning</title>
      <link>https://varun19299.github.io/talk/advances_in_theory_dl/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/talk/advances_in_theory_dl/</guid>
      <description>

&lt;h2 id=&#34;randomly-initialised-infinite-deep-networks-as-gps&#34;&gt;Randomly Initialised Infinite Deep Networks as GPs&lt;/h2&gt;

&lt;h2 id=&#34;infinite-channel-cnns-are-gps&#34;&gt;Infinite Channel CNNs are GPs&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>MRAS Bandits: From Control Theory to Slot Machines</title>
      <link>https://varun19299.github.io/research/mras/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/research/mras/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note: This work was done as a part of the course project under the guidance of &lt;a href=&#34;http://www.cse.iitm.ac.in/~prashla/&#34; target=&#34;_blank&#34;&gt;Prof. LA Prashanth&lt;/a&gt; for CS6700, IIT Madras. We are presently working on finite time analysis of this algorithm for both Regret Minimisation and Best Arm Identification settings.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-mras-algorithm&#34;&gt;The MRAS Algorithm&lt;/h3&gt;

&lt;h3 id=&#34;adapting-to-the-bandit-setting&#34;&gt;Adapting to the Bandit Setting&lt;/h3&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Yue &lt;em&gt;et al.&lt;/em&gt;, &lt;strong&gt;A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving&lt;/strong&gt;, CVPR 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla docs&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references-berkeley/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references-berkeley/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra, &lt;strong&gt;“A Unified Framework for Lensless Image Recovery&lt;/strong&gt;, to be submitted in the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nick Antipa, Patrick Oare, Emrah Bostan, Ren Ng and Laura Waller. Video from Stills: Lensless Imaging with Rolling Shutter. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 5. [Attribute Transfer Blog Post](/research/attribute-transfer).
   
1. [MRAS Bandits Blog Post](/research/mras). --&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references-stanford/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references-stanford/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra, &lt;strong&gt;“A Unified Framework for Lensless Image Recovery”&lt;/strong&gt;, to be submitted in the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Julie Chang and Gordon Wetzstein - Deep Optics for Monocular Depth Estimation and 3D Object Detection. In IEEE International Conference on Computer Vision (ICCV), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 5. [Attribute Transfer Blog Post](/research/attribute-transfer).
   
1. [MRAS Bandits Blog Post](/research/mras). --&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra, &lt;strong&gt;“A Unified Framework for Lensless Image Recovery”&lt;/strong&gt;, to be submitted in the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 5. [Attribute Transfer Blog Post](/research/attribute-transfer).
   
1. [MRAS Bandits Blog Post](/research/mras). --&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Commercial LiDARs using Physics based Game Engines</title>
      <link>https://varun19299.github.io/projects/lidar/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/projects/lidar/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note: This work was done as a part of the &lt;strong&gt;MBRDI Hackathon 2019, Bangalore&lt;/strong&gt;. We placed third in the contest with our entry. For further details you can refer to our technical report or presentations.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;LiDAR point clouds are crucial in achieving level 4 and level 5 autonomous navigation. At the same time, obtaining LiDAR data for extensive scenes can be quite challenging. Hence, generating LiDAR data under synthetic simulations with control on parameters such as reflectivity, range, Field of View (FOV), etc would be useful. (See &lt;a href=&#34;https://www.youtube.com/watch?v=oZ7P4RsTE64&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; if you would like to see a how real LiDAR outputs look like.)&lt;/p&gt;

&lt;p&gt;In this project, we demonstrate an approach based on &lt;strong&gt;treating a camera as the equivalent of a LiDAR.&lt;/strong&gt; In fact, this approach has been used elsewhere in computational photography and photogrammetry (or geometric computer vision): a very similar concept is prevelant in the very construction of a Time of Flight camera.&lt;/p&gt;

&lt;p&gt;A careful reader at this point would interrupt us to ask a natural question. Why not consider ray-casting, which is offered by many physics and game engines alike? Why then should someone choose to resort to such a camera based approach? With all the existing work behind parallel compute for ray casting, it seems like the perfect candidate for the job. The picture at the top represents this, and is in fact from the simulation engine &lt;a href=&#34;http://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla&lt;/a&gt;. However, the team at &lt;strong&gt;MBRDI&lt;/strong&gt; wanted us to generate point clouds for a given road layout and for &lt;strong&gt;specified objects only&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;opendrive-layout&#34;&gt;OpenDRIVE Layout&lt;/h3&gt;

&lt;p&gt;The input data given is in the form of an &lt;a href=&#34;http://www.opendrive.org&#34; target=&#34;_blank&#34;&gt;OpenDRIVE&lt;/a&gt; layout, which essentially specifies road geometry and other objects (crossings, potential obstacles, poles, etc.). It &lt;strong&gt;does not&lt;/strong&gt; include information on road texture or renderings of objects present. In the below picture, we show a sample Opendrive file.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opendrive.png&#34; data-caption=&#34;Sample OpenDRIVE layout.&#34;&gt;
&lt;img src=&#34;opendrive.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;400&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample OpenDRIVE layout.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Note the same file rendered with software such as Sketchup 3D.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;sketch.png&#34; data-caption=&#34;Same OpenDRIVE layout rendered by Sketch3D.&#34;&gt;
&lt;img src=&#34;sketch.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Same OpenDRIVE layout rendered by Sketch3D.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;So, we wish to create a blackbox when given this road layout (and optionally some knowledge regarding objects on the road) can generate a 3D point cloud for specified targets, closely mimicking a real LiDAR.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;flow.png&#34; data-caption=&#34;Intended process flowchart.&#34;&gt;
&lt;img src=&#34;flow.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;400&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Intended process flowchart.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;image-lidar-calibration&#34;&gt;Image-LIDAR Calibration&lt;/h3&gt;

&lt;p&gt;Our first step is to draw correspondence between a given pixel and a 3D world point. This is done in a straightforward manner using projective geometry.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;proj.png&#34; data-caption=&#34;Proposed calibration scheme between a LiDAR and a camera. Credits: Yue et al., CVPR&amp;rsquo;18.&#34;&gt;
&lt;img src=&#34;proj.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Proposed calibration scheme between a LiDAR and a camera. Credits: Yue &lt;em&gt;et al.&lt;/em&gt;, CVPR&amp;rsquo;18.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;The goal of the calibration process is to find a function between each LiDAR Point and a pixel in the image. By utilising a depth map, we are able to do the converse too. The method we have adopted does the calibration automatically based on camera and LiDAR scanner parameters. It is similar to the camera perspective projection model once we set the camera and LiDAR at the same position, as shown in the above figure. Our method is based on work by Yue &lt;em&gt;et al.&lt;/em&gt;, CVPR 2018. We utilise their calibration scheme for our task as well.&lt;/p&gt;

&lt;p&gt;The problem is formulated as follows: for a certain laser ray  with azimuth angle &lt;code&gt;$\phi$&lt;/code&gt;  and zenith angle &lt;code&gt;$\theta$&lt;/code&gt;, calculate the  index &lt;code&gt;$(i,j)$&lt;/code&gt; of  the  corresponding  pixel  on  image. &lt;code&gt;${F_c},{F_o},{P}, {P^{\prime}}$&lt;/code&gt; and &lt;code&gt;$P_{far}$&lt;/code&gt; are  3-D  coordinates  of  a)  centre  of camera/LiDAR  scanner,  b)  centre  of  camera  near  clipping plane.  c)  point  first  hit  by  the  virtual  laser  ray  (in  red), d)  pixel  on  image  corresponding  to &lt;code&gt;$P$&lt;/code&gt;, e)  a  point  far away  in  the  laser  direction,  respectively. Also, &lt;code&gt;$m$&lt;/code&gt; and &lt;code&gt;$n$&lt;/code&gt; are  the width and height of the near clipping plane. &lt;code&gt;$\gamma$&lt;/code&gt; is &lt;code&gt;$1/2$&lt;/code&gt; vertical FOV of camera while &lt;code&gt;$\psi$&lt;/code&gt; is &lt;code&gt;$1/2$&lt;/code&gt; vertical FOV of the LiDAR scanner.  Note  that  LiDAR  scanner  FOV  is usually  smaller than camera FOV, since there is usually no object in the top part of the image, and the emitting laser to open space is not necessary.&lt;/p&gt;

&lt;p&gt;After a series of 3D geometry calculation, we can get:
&lt;code&gt;$$i = \frac{R_m}{m} (f\cdot tan\gamma \cdot \frac{m}{n} - \frac{f}{cos\theta} \cdot tan\phi)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$j = \frac{R_n}{n} (f\cdot tan\gamma + f \cdot tan\theta)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where &lt;code&gt;$f = \||\vec{F_c F_o}||$&lt;/code&gt;, and &lt;code&gt;$(R_m,R_n)$&lt;/code&gt; is the pixel resolution of the image/near clipping plane. Further, in order for the ray casting API to work properly, the 3D coordinates of &lt;code&gt;$P_{far}$&lt;/code&gt; are also required. Using similar 3D geometry calculations, we obtain:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$P^{\prime} = F_c + f \cdot \vec{x_c} - \frac{f}{cos\theta} \cdot tan \phi \cdot \vec{y_c} -f\cdot tan\theta \cdot \vec{z_c}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$P_{far}= F_c + k \cdot (P^{\prime}- F_c)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where k is a large coefficient, and &lt;code&gt;$\vec{x_c}, \vec{y_c}, \vec{z_c}$&lt;/code&gt; are unit vectors of the camera axis in the world coordinate system. After simulation, both image and point cloud of the specified in-model scene are collected by the framework.&lt;/p&gt;

&lt;p&gt;With this step, we can associate each 3D point to a pixel and each (valid) pixel to a given 3D point according to the LiDAR&amp;rsquo;s construction.&lt;/p&gt;

&lt;h3 id=&#34;identify-targets-introduce-sparsity&#34;&gt;Identify Targets, Introduce Sparsity&lt;/h3&gt;

&lt;p&gt;The second step involves introducing artefacts such as sparsity and intensity variation typically found in real LiDARs such as a &lt;a href=&#34;http://velodynelidar.com/&#34; target=&#34;_blank&#34;&gt;Velodyne VLP-16&lt;/a&gt;. We can either do this by modelling the physics for reflectivity (with models of varying complexity) or by using data as a prior. Since our purpose here is to just demonstrate a proof of concept, we take the easier way out and turn to data-driven priors. In essence, we utilise a deep network which converts image taken by a camera to a sparse intensity map akin to a LiDAR.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;kitti.png&#34; data-caption=&#34;Sample sparse depth points from the KITTI Dataset.&#34;&gt;
&lt;img src=&#34;kitti.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample sparse depth points from the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/&#34; target=&#34;_blank&#34;&gt;KITTI Dataset&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;For the required data, we turn to the popular KITTI dataset, which includes VLP-16 acquisitions as shown below. For a visualisation of LiDAR data in the KITTI dataset, take a look at this excellant &lt;a href=&#34;https://navoshta.com/kitti-lidar/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;. A sample data point is shown in the figure above. In this step, we also choose to identify targets of interest according to the given road layout (which is in the OpenDrive format).&lt;/p&gt;

&lt;h3 id=&#34;reproject-points-back-to-3d&#34;&gt;Reproject Points back to 3D&lt;/h3&gt;

&lt;p&gt;Finally, having selected our objects of interest in the camera frame, we can reproject back to 3D. We select these objects using routine tools from computer vision such as segmentation, contour detection, object detection etc. In order to obtain intensities at those points, we simply follow a naive inverse-square power law model. Here&amp;rsquo;s a sample point cloud rendering.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;point_cloud.png&#34; data-caption=&#34;Sample point cloud.&#34;&gt;
&lt;img src=&#34;point_cloud.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample point cloud.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;concluding-notes&#34;&gt;Concluding Notes&lt;/h3&gt;

&lt;p&gt;Our solution to this hackathon conducted by &lt;strong&gt;MBRDI&lt;/strong&gt; involved a simple bijection  between LiDARs and cameras in the simulation world. With this correspondence, we are oepn to a lot of tools from the image processing and computer vision community. We&amp;rsquo;ve certainly been sub-optimal in a lot of places, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simple assumptions on intensity variation.&lt;/li&gt;
&lt;li&gt;Not handling multiple or specular reflections.&lt;/li&gt;
&lt;li&gt;Point clouds rendered is with respect to texture offered by such game engines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We shall try addressing some of these as future work.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Yue &lt;em&gt;et al.&lt;/em&gt;, &lt;strong&gt;A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving&lt;/strong&gt;, CVPR 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla docs&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references-caltech/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references-caltech/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra, &lt;strong&gt;“A Unified Framework for Lensless Image Recovery&lt;/strong&gt;, to be submitted in the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Katherine L. Bouman, Vickie Ye, Adam B. Yedidia, Fre ́do Durand, Gregory W. Wornell, Antonio Torralba and William T. Freeman. Turning Corners into Cameras: Principles and Methods. In International Conference on Computer Vision (ICCV), 2017.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tianfan Xue, Jiajun Wu, Katherine L. Bouman and William T. Freeman. Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks. In Advances In Neural Information Processing Systems (NeurIPS) 2016.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 5. [Attribute Transfer Blog Post](/research/attribute-transfer).
   
1. [MRAS Bandits Blog Post](/research/mras). --&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references-usc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references-usc/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra, &lt;strong&gt;“A Unified Framework for Lensless Image Recovery&lt;/strong&gt;, to be submitted in the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A Sadhu, K Chen and R Nevatia. Zero-Shot Grounding of Objects from Natural Language Queries. In IEEE International Conference on Computer Vision (ICCV), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;T Xue, J Wu, K Bouman and W Freeman. Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks. In Neural Information Processing Systems (NeurIPS), 2016.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;R Ge, J Gao, K Chen and R Nevatia. MAC: Mining Activity Concepts for Language-based Temporal Localization. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2019.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
