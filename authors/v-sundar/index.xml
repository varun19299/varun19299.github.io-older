<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>V Sundar | Varun Sundar</title>
    <link>https://varun19299.github.io/authors/v-sundar/</link>
      <atom:link href="https://varun19299.github.io/authors/v-sundar/index.xml" rel="self" type="application/rss+xml" />
    <description>V Sundar</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://varun19299.github.io/img/icon-192.png</url>
      <title>V Sundar</title>
      <link>https://varun19299.github.io/authors/v-sundar/</link>
    </image>
    
    <item>
      <title>Extending LSD SLAM with Computational Photography</title>
      <link>https://varun19299.github.io/post/extend-lsd-slam/</link>
      <pubDate>Thu, 28 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/post/extend-lsd-slam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Attribute Transfer in a GAN Framework</title>
      <link>https://varun19299.github.io/research/attribute-transfer/</link>
      <pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/research/attribute-transfer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MRAS Bandits: From Control Theory to Slot Machines</title>
      <link>https://varun19299.github.io/research/mras/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/research/mras/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note: This work was done as a part of the course project under the guidance of &lt;a href=&#34;http://www.cse.iitm.ac.in/~prashla/&#34; target=&#34;_blank&#34;&gt;Prof. LA Prashanth&lt;/a&gt; for CS6700, IIT Madras. We are presently working on finite time analysis of this algorithm for both Regret Minimisation and Best Arm Identification settings.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-mras-algorithm&#34;&gt;The MRAS Algorithm&lt;/h3&gt;

&lt;h3 id=&#34;adapting-to-the-bandit-setting&#34;&gt;Adapting to the Bandit Setting&lt;/h3&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Yue &lt;em&gt;et al.&lt;/em&gt;, &lt;strong&gt;A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving&lt;/strong&gt;, CVPR 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla docs&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra, &lt;strong&gt;“Unified Image Recovery for Lensless Systems”&lt;/strong&gt;, to be submitted in the IEEE Transactions on Computational Imaging 2020.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cfi.iitm.ac.in/wordpress/&#34; target=&#34;_blank&#34;&gt;Centre for Innovation, IIT Madras&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 5. [Attribute Transfer Blog Post](/research/attribute-transfer).
   
1. [MRAS Bandits Blog Post](/research/mras). --&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Commercial LiDARs using Physics based Game Engines</title>
      <link>https://varun19299.github.io/projects/lidar/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/projects/lidar/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note: This work was done as a part of the &lt;strong&gt;MBRDI Hackathon 2019, Bangalore&lt;/strong&gt;. We placed third in the contest with our entry. For further details you can refer to our technical report or presentations.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;LiDAR point clouds are crucial in achieving level 4 and level 5 autonomous navigation. At the same time, obtaining LiDAR data for extensive scenes can be quite challenging. Hence, generating LiDAR data under synthetic simulations with control on parameters such as reflectivity, range, Field of View (FOV), etc would be useful. (See &lt;a href=&#34;https://www.youtube.com/watch?v=oZ7P4RsTE64&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; if you would like to see a how real LiDAR outputs look like.)&lt;/p&gt;

&lt;p&gt;In this project, we demonstrate an approach based on &lt;strong&gt;treating a camera as the equivalent of a LiDAR.&lt;/strong&gt; In fact, this approach has been used elsewhere in computational photography and photogrammetry (or geometric computer vision): a very similar concept is prevelant in the very construction of a Time of Flight camera.&lt;/p&gt;

&lt;p&gt;A careful reader at this point would interrupt us to ask a natural question. Why not consider ray-casting, which is offered by many physics and game engines alike? Why then should someone choose to resort to such a camera based approach? With all the existing work behind parallel compute for ray casting, it seems like the perfect candidate for the job. The picture at the top represents this, and is in fact from the simulation engine &lt;a href=&#34;http://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla&lt;/a&gt;. However, the team at &lt;strong&gt;MBRDI&lt;/strong&gt; wanted us to generate point clouds for a given road layout and for &lt;strong&gt;specified objects only&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;opendrive-layout&#34;&gt;OpenDRIVE Layout&lt;/h3&gt;

&lt;p&gt;The input data given is in the form of an &lt;a href=&#34;http://www.opendrive.org&#34; target=&#34;_blank&#34;&gt;OpenDRIVE&lt;/a&gt; layout, which essentially specifies road geometry and other objects (crossings, potential obstacles, poles, etc.). It &lt;strong&gt;does not&lt;/strong&gt; include information on road texture or renderings of objects present. In the below picture, we show a sample Opendrive file.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opendrive.png&#34; data-caption=&#34;Sample OpenDRIVE layout.&#34;&gt;
&lt;img src=&#34;opendrive.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;400&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample OpenDRIVE layout.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Note the same file rendered with software such as Sketchup 3D.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;sketch.png&#34; data-caption=&#34;Same OpenDRIVE layout rendered by Sketch3D.&#34;&gt;
&lt;img src=&#34;sketch.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Same OpenDRIVE layout rendered by Sketch3D.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;So, we wish to create a blackbox when given this road layout (and optionally some knowledge regarding objects on the road) can generate a 3D point cloud for specified targets, closely mimicking a real LiDAR.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;flow.png&#34; data-caption=&#34;Intended process flowchart.&#34;&gt;
&lt;img src=&#34;flow.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;400&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Intended process flowchart.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;image-lidar-calibration&#34;&gt;Image-LIDAR Calibration&lt;/h3&gt;

&lt;p&gt;Our first step is to draw correspondence between a given pixel and a 3D world point. This is done in a straightforward manner using projective geometry.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;proj.png&#34; data-caption=&#34;Proposed calibration scheme between a LiDAR and a camera. Credits: Yue et al., CVPR&amp;rsquo;18.&#34;&gt;
&lt;img src=&#34;proj.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Proposed calibration scheme between a LiDAR and a camera. Credits: Yue &lt;em&gt;et al.&lt;/em&gt;, CVPR&amp;rsquo;18.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;The goal of the calibration process is to find a function between each LiDAR Point and a pixel in the image. By utilising a depth map, we are able to do the converse too. The method we have adopted does the calibration automatically based on camera and LiDAR scanner parameters. It is similar to the camera perspective projection model once we set the camera and LiDAR at the same position, as shown in the above figure. Our method is based on work by Yue &lt;em&gt;et al.&lt;/em&gt;, CVPR 2018. We utilise their calibration scheme for our task as well.&lt;/p&gt;

&lt;p&gt;The problem is formulated as follows: for a certain laser ray  with azimuth angle &lt;code&gt;$\phi$&lt;/code&gt;  and zenith angle &lt;code&gt;$\theta$&lt;/code&gt;, calculate the  index &lt;code&gt;$(i,j)$&lt;/code&gt; of  the  corresponding  pixel  on  image. &lt;code&gt;${F_c},{F_o},{P}, {P^{\prime}}$&lt;/code&gt; and &lt;code&gt;$P_{far}$&lt;/code&gt; are  3-D  coordinates  of  a)  centre  of camera/LiDAR  scanner,  b)  centre  of  camera  near  clipping plane.  c)  point  first  hit  by  the  virtual  laser  ray  (in  red), d)  pixel  on  image  corresponding  to &lt;code&gt;$P$&lt;/code&gt;, e)  a  point  far away  in  the  laser  direction,  respectively. Also, &lt;code&gt;$m$&lt;/code&gt; and &lt;code&gt;$n$&lt;/code&gt; are  the width and height of the near clipping plane. &lt;code&gt;$\gamma$&lt;/code&gt; is &lt;code&gt;$1/2$&lt;/code&gt; vertical FOV of camera while &lt;code&gt;$\psi$&lt;/code&gt; is &lt;code&gt;$1/2$&lt;/code&gt; vertical FOV of the LiDAR scanner.  Note  that  LiDAR  scanner  FOV  is usually  smaller than camera FOV, since there is usually no object in the top part of the image, and the emitting laser to open space is not necessary.&lt;/p&gt;

&lt;p&gt;After a series of 3D geometry calculation, we can get:
&lt;code&gt;$$i = \frac{R_m}{m} (f\cdot tan\gamma \cdot \frac{m}{n} - \frac{f}{cos\theta} \cdot tan\phi)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$j = \frac{R_n}{n} (f\cdot tan\gamma + f \cdot tan\theta)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where &lt;code&gt;$f = \||\vec{F_c F_o}||$&lt;/code&gt;, and &lt;code&gt;$(R_m,R_n)$&lt;/code&gt; is the pixel resolution of the image/near clipping plane. Further, in order for the ray casting API to work properly, the 3D coordinates of &lt;code&gt;$P_{far}$&lt;/code&gt; are also required. Using similar 3D geometry calculations, we obtain:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$P^{\prime} = F_c + f \cdot \vec{x_c} - \frac{f}{cos\theta} \cdot tan \phi \cdot \vec{y_c} -f\cdot tan\theta \cdot \vec{z_c}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$P_{far}= F_c + k \cdot (P^{\prime}- F_c)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where k is a large coefficient, and &lt;code&gt;$\vec{x_c}, \vec{y_c}, \vec{z_c}$&lt;/code&gt; are unit vectors of the camera axis in the world coordinate system. After simulation, both image and point cloud of the specified in-model scene are collected by the framework.&lt;/p&gt;

&lt;p&gt;With this step, we can associate each 3D point to a pixel and each (valid) pixel to a given 3D point according to the LiDAR&amp;rsquo;s construction.&lt;/p&gt;

&lt;h3 id=&#34;identify-targets-introduce-sparsity&#34;&gt;Identify Targets, Introduce Sparsity&lt;/h3&gt;

&lt;p&gt;The second step involves introducing artefacts such as sparsity and intensity variation typically found in real LiDARs such as a &lt;a href=&#34;http://velodynelidar.com/&#34; target=&#34;_blank&#34;&gt;Velodyne VLP-16&lt;/a&gt;. We can either do this by modelling the physics for reflectivity (with models of varying complexity) or by using data as a prior. Since our purpose here is to just demonstrate a proof of concept, we take the easier way out and turn to data-driven priors. In essence, we utilise a deep network which converts image taken by a camera to a sparse intensity map akin to a LiDAR.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;kitti.png&#34; data-caption=&#34;Sample sparse depth points from the KITTI Dataset.&#34;&gt;
&lt;img src=&#34;kitti.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample sparse depth points from the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/&#34; target=&#34;_blank&#34;&gt;KITTI Dataset&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;For the required data, we turn to the popular KITTI dataset, which includes VLP-16 acquisitions as shown below. For a visualisation of LiDAR data in the KITTI dataset, take a look at this excellant &lt;a href=&#34;https://navoshta.com/kitti-lidar/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;. A sample data point is shown in the figure above. In this step, we also choose to identify targets of interest according to the given road layout (which is in the OpenDrive format).&lt;/p&gt;

&lt;h3 id=&#34;reproject-points-back-to-3d&#34;&gt;Reproject Points back to 3D&lt;/h3&gt;

&lt;p&gt;Finally, having selected our objects of interest in the camera frame, we can reproject back to 3D. We select these objects using routine tools from computer vision such as segmentation, contour detection, object detection etc. In order to obtain intensities at those points, we simply follow a naive inverse-square power law model. Here&amp;rsquo;s a sample point cloud rendering.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;point_cloud.png&#34; data-caption=&#34;Sample point cloud.&#34;&gt;
&lt;img src=&#34;point_cloud.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample point cloud.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;concluding-notes&#34;&gt;Concluding Notes&lt;/h3&gt;

&lt;p&gt;Our solution to this hackathon conducted by &lt;strong&gt;MBRDI&lt;/strong&gt; involved a simple bijection  between LiDARs and cameras in the simulation world. With this correspondence, we are oepn to a lot of tools from the image processing and computer vision community. We&amp;rsquo;ve certainly been sub-optimal in a lot of places, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simple assumptions on intensity variation.&lt;/li&gt;
&lt;li&gt;Not handling multiple or specular reflections.&lt;/li&gt;
&lt;li&gt;Point clouds rendered is with respect to texture offered by such game engines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We shall try addressing some of these as future work.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Yue &lt;em&gt;et al.&lt;/em&gt;, &lt;strong&gt;A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving&lt;/strong&gt;, CVPR 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla docs&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
