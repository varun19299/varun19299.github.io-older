[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a senior undergraduate in the Department of Electrical Engineering at the Indian Institute of Technology Madras (IIT-M), India. My goal is to pursue a PhD in the intersection of computer vision and computational photography, begining Fall 2020.\nThis website is a collection of my research projects and other exploratory projects done. There is also a solitary blog-post regarding a pet research idea of mine: CNN-SLAM. Hopefully, there\u0026rsquo;ll be a few others joining this one soon!\nIn case any of these topics piques your interests and you would like to talk about them (or just say hi) feel free to contact me!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1576048895,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://varun19299.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a senior undergraduate in the Department of Electrical Engineering at the Indian Institute of Technology Madras (IIT-M), India. My goal is to pursue a PhD in the intersection of computer vision and computational photography, begining Fall 2020.\nThis website is a collection of my research projects and other exploratory projects done. There is also a solitary blog-post regarding a pet research idea of mine: CNN-SLAM. Hopefully, there\u0026rsquo;ll be a few others joining this one soon!","tags":null,"title":"Varun Sundar","type":"authors"},{"authors":["V Sundar"],"categories":null,"content":" Varun Sundar, Salman Siddique and Kaushik Mitra, “A Unified Framework for Lensless Image Recovery”, to be submitted in the IEEE Transactions on Computational Imaging 2020.\n The 9th HULT Prize, Singapore. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.\n H Drew and M Christopher. GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n Varun Sundar, Arka Sadhu and Ram Nevatia, “FARCNN: Decoupling attributes from object detection”, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.\n Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.\n Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.\n Blog Post on extending LSD SLAM.\n Centre for Innovation, IIT Madras.\n Tobias Gruber, Frank D. Julca-Aguilar, Mario Bijelic, Werner Ritter, Klaus Dietmayer, and Felix Heide. Gated2depth: Real-time dense lidar from gated images. CoRR, abs/1902.04997, 2019.\n  ","date":1576022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576022400,"objectID":"d6ddac469dd42c18a7f39499e8f01357","permalink":"https://varun19299.github.io/sop-references-princeton/","publishdate":"2019-12-11T00:00:00Z","relpermalink":"/sop-references-princeton/","section":"","summary":"References used in my statement of purpose","tags":["SOP References"],"title":"References: Statement of Purpose","type":"page"},{"authors":["V Sundar"],"categories":null,"content":" H Drew and M Christopher. GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n V Sundar, A Sadhu, R Nevatia. FARCNN: Decoupling attributes from object detection. In Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.\n V Sundar, S Siddique, K Mitra. A Unified Framework for Lensless Image Recovery”. To be submitted in the IEEE Transactions on Computational Imaging 2020.\n A Kadambi, V Taamazyan, B Shi, and R Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.\n A Kadambi and R Raskar. Rethinking Machine Vision Time of Flight With GHz Heterodyning. In IEEE Access, 2017.\n  ","date":1576022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"80fdbc957c398bd506854e8bf1f51f16","permalink":"https://varun19299.github.io/sop-references-ucla/","publishdate":"2019-12-11T00:00:00Z","relpermalink":"/sop-references-ucla/","section":"","summary":"References used in my statement of purpose","tags":["SOP References"],"title":"References: Statement of Purpose","type":"page"},{"authors":["V Sundar"],"categories":null,"content":" In this post, we shall explore the key steps of Large Scale Direct SLAM (LSD SLAM) and understand its main strengths and weaknesses. We shall also see how we can possibly overcome some of these limitations with recent advances in Computational Photography.\nWhat is LSD SLAM? LSD SLAM stands for Large Scale Direct SLAM, implying that it performs simultaneous localisation and mapping directly on the entire set of image intensities, instead of relying on a subset of points such as SIFT, FAST or ORB features. Essentially, this allows LSD SLAM to utilise all the information available in the image and thereby not discard crucial information contained in edges, which constitutes a major portion of the scene being mapped.\nWhere LSD SLAM crucially differs from earlier direct SLAM methods \\cite, is the ingenious method of tracking and depth estimation. Depth estimation is done via small baseline stereo which allows for much fewer outliers. Tracking is done using direct image alignment over a subset of frames. The main novelty in this work include:\n utilising key-frames to simplify monocular SLAM. utilising a scale-aware image alignment algorithm to directly estimate the similarity transform $\\xi ∈ \\mathcal{sim}(3)$ between two keyframes. Incorporation of uncertainty of the estimated depth into tracking.  Here, $\\mathcal{sim}-3$ is the lie algebra of the Special Euclidean Objects group (SE-3). A lie algebra offers easier optimisation of variables by linearising the manifold of consideration. Intuitively, if we consider just rotation as a matrix, then it must be constrained to be orthogonal ($U^TU=I$). However, if we were optimising it with, say, gradient descent, then there is no guarantee that the resulting matrix would be orthogonal. Further, we only want to optimise on a subset of $\\mathbb{R}^{3\\times 3}$. Lie algebras encapsulate this effectively: the resultant $\\mathcal{sim}-3 \\in \\mathbb{R}^6$ can be optimised by gradient descent. For more details, we would refer you to (2).\nMain Pipeline    Overview of LSD SLAM.   The pipeline follows: 1. Decide whether to initialise a key-frame 2. Compute incoming pose via direct image alignment 3. Compute depth via small baseline stereo 4. Repeat, finally perform map optimisation to correct for scale ambiguity\nWe shall elaborate on each of these main steps.\nConcept of Keyframes As mentioned before, LSD SLAM heavily utilises the concept of keyframes: \u0026ldquo;special\u0026rdquo; frames, which are initialised when a new frame has a pose significantly different from the current keyframe. The very first frame is taken to be a key-frame, with random normal depth, and random normal depth uncertainity.\nMap optimisation is performed only on the set of key-frames, further lowering computational costs associated with bundle adjustment and loop closure.\nDirect Image Alignment Limitations of LSD SLAM Scale Ambiguity Pure Rotation Settings Can Computational Photography help us? References  A Jose Blanco, \u0026ldquo;A tutorial on SE(3) transformation parameterizations and on-manifold optimization\u0026rdquo;, MAPIR Group.  ","date":1574899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"2ce2b0a2f41d9cc2aa828342584c12da","permalink":"https://varun19299.github.io/post/extend-lsd-slam/","publishdate":"2019-11-28T00:00:00Z","relpermalink":"/post/extend-lsd-slam/","section":"post","summary":"Blog Post explaining the pipeline of Large Scale Direct (LSD) SLAM, its limitations and how recent advances in Computational Photography can help alleviate some of them. ","tags":["Project"],"title":"Extending LSD SLAM with Computational Photography","type":"post"},{"authors":["V Sundar"],"categories":null,"content":"","date":1574208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"9ad405ff7df06e087c912a4f83bb54db","permalink":"https://varun19299.github.io/research/attribute-transfer/","publishdate":"2019-11-20T00:00:00Z","relpermalink":"/research/attribute-transfer/","section":"research","summary":"Blog Post explaining my research on Attribute Transfer in an Image Manipulation setting.","tags":["Research"],"title":"Attribute Transfer in a GAN Framework","type":"research"},{"authors":["V Sundar"],"categories":null,"content":" Randomly Initialised Infinite Deep Networks as GPs Infinite Channel CNNs are GPs ","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"2959f4c0b83e05405833241ce3243ac9","permalink":"https://varun19299.github.io/talk/advances_in_theory_dl/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/talk/advances_in_theory_dl/","section":"talk","summary":"Presentations made as a part of CS7020: Advances in the Theory of Deep Learning, Fall 2019.","tags":["Presentations","Talks"],"title":"Presentations in CS7020: Advances in the Theory of Deep Learning","type":"talk"},{"authors":null,"categories":null,"content":"You can write to me at [varun19299] AT [gmail.com]. I\u0026rsquo;m not very active on social media, besides the occasional glance at twitter or reddit. My twitter handle is @varun19299.\n","date":1570320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://varun19299.github.io/contact/","publishdate":"2019-10-06T00:00:00Z","relpermalink":"/contact/","section":"","summary":"You can write to me at [varun19299] AT [gmail.com]. I\u0026rsquo;m not very active on social media, besides the occasional glance at twitter or reddit. My twitter handle is @varun19299.","tags":null,"title":"Contact Me","type":"page"},{"authors":null,"categories":null,"content":" Note: This blog post is meant to serve as a journal or a log of sorts of my learnings and reflections in a course I took up this semester (Fall 2019). Technically, owing to its sparse insertions, it is barely a journal, maybe more of a reflection. And certainly no memoir! It is not meant to be a reliable course transcript and as such might skim on some portions, while emphasising on others.\nTable of Contents  Table of Contents Search for Purpose Mindfulness Understanding your Personality  Search for Purpose In this introductory module, we began by discussing our expectations and aspirations from the course. For me, I wanted to use this course as a much needed opportunity for introspection, perhaps meet a few new people (at insti, you can never have met everyone!) and may be some wise take-home advice would be an added bonus. Edit, 19th November, 2019: After spending a semester in the course, I can say I received a lot more than my initial objectives, and I\u0026rsquo;m glad about it.\nWe began with the goal of visioning with a little help from a certain Eastern concept. Ikigai, roughly translating to \u0026ldquo;thing that you live for\u0026rdquo;, is the Japanese concept of finidng a purpose in life. The philosphy revolves around the idea that once you discover your true, unwaivering purpose in life, the journey called life becomes much more meaningful. This philosophy historically originates as a ritualisitc practice from the southern most prefecture of Japan, Okinawa on the islands of Ryuku. For long, the inhabitants of the island have been quentissentially associated with the lack of any desire to retire. They continue, as long as their physical bodies permit, doing their favourtie activities. In fact they wake up everyday with the desire to perform that activity. Which is quite ironic compared to our modern lives, where people as early as their 30\u0026rsquo;s start thinking about retirement!\nIn a famous TED Talk (inset below), National Geographic reporter Dan Buettner posited this existence of a definite reason to be the cause for the long lifespans of the island\u0026rsquo;s inhabitants. This sort of feels intuitive to me (and hopefully to a lot of others too!). Having a purpose, something you wake up to each day, can be pretty satisfying. And that often translates to happiness and general well-being. In the course, taking inspiration from the same, we set out on a small exercise to find our \u0026ldquo;purpose\u0026rdquo; in life.\n  At the onset, this did not make much sense to me. Yes, goal making is important and is quite often stressed in many work-ethic related workshops. But there\u0026rsquo;s one caveat to goal making: finding the right goal, especially since it is not trivial to understand what is relevant. Luckily, the ikigai addresses this too, by asking four insightful questions. It asks first what you love to do, something which gives you joy of some form or doesn\u0026rsquo;t feel like a burden when you do it. Second, what is it that you do well. This could include activities in which you are skilled or talented. You may or may not derive satisfaction out of this. Intersecting these two question should give you a list of things you\u0026rsquo;re passionate about.\n   Notice that this question isn\u0026rsquo;t concerned with how others perceive your tasks. Well, that\u0026rsquo;s the third. What are things you do that others value? This could be monetary value, intellectual value or even a non-materialistic value. But it might be easier for you to relate to this question if you rephrase it as: \u0026ldquo;what is it that people are willing to pay me for?\u0026rdquo;. Here, the intersection of the second and third questions give you a profession: something you are skilled at, and is valuable to others. The final question, and maybe the most important of them, is: what does the world require now? The answer to this question allows us to rise above just ourselves and contribute to the larger cause of humanity. For those of you who are fortunate to have activities at the intersection at questions one and four, voila, you have a mission.\nFinally, ikigai is the harmonious synergy of all these four questions. In other words it urges you to find an activity which could be a profession, passion, vocation and mission for you at the same time. Pretty confounding if you ask me. Now it might not be easy to find this sweet spot in one go. It might take quite a while actually. But the journey involved in finding your ikigai can be quite satisfying. It can also serve as a guiding light for rethinking some of your favourite activities. Say you like teaching and see it as a viable profession: maybe 5 or 10 years down the line. Now how would we transform this into something the world needs. Well it so happens that quality education is number 4 on the UN Sustainable Development Goals (SDGs) list. Off the top of my head, you could either work on making education more accessible to the global population - there are over 115 million without access to quality primary education - or work on effective communication and disttilation of knowledge (which in my opinion, is pretty important too, read this for a comprehensive article on how lack of distilling knowledge plagues schools and academia alike).\nReading Suggestions:\n For an interesting take on finding meaningful work and whether \u0026ldquo;the one\u0026rdquo; exists (no this is not relationship advice): Adam “Smiley” Poswolsky on How to Find Meaningful Work in the Age of Job-Hopping. If Forbes is your thing: Discover Your Passion \u0026ndash; Or \u0026lsquo;Ikigai\u0026rsquo; \u0026ndash; With 4 Simple Tips. An empirical study on the importance of theoretical work: Meaningful Work, by Andrea Veltman.  Mindfulness Another module which deeply resonanted with me is that of mindfulness. My earliest exposure to the concept was when I picked up a book from Mom\u0026rsquo;s reading stack one afternoon and as luck would have it, I ran into the Power of Now by Ekhard Tole. The book isn\u0026rsquo;t dreadful, but it certainly was heavy for a school kid. Yet, there were a few ideas from the book that I actually did like. The idea of turning off our minds completely for a while everyday and becoming passive observers of our thought process: two very calming rituals.\nMindfulness is a more precise term for this broad idea of living more actively in the present, noticing the smaller details around us. It could be as simple as stopping by to admire the fresh dew on a cold morning, or noticing how that stray cat does a weird movement with its ears everytime it gets up. But the idea is to observe, observe both passively and actively. Active in your senses, and passive so as to not let your biases and notions cloud the scene. If this sounds to you like a line Oogway from Kung Fu Panda would say, you\u0026rsquo;re probably on the right track. And since we brought up Oogway, I think this line from the movie is pretty apt in this context:\n \u0026ldquo;Yesterday is history, tomorrow is a mystery, and today is a gift\u0026hellip; that\u0026rsquo;s why they call it the present”\n    Oogway spreading his wisdom, circa 2010.   Although seemingly unconnected, I feel mindfulness shares a lot of concepts in common with other practices including time management and deliberate practice (or deep work). Time Management, a concept, dreaded by procrastinators and capitalised by life-coaches, it certainly goes hand-in-hand with mindfulness. Most time-management practices involve to varying degrees the art of prioritizing, dividing your day into \u0026ldquo;phases\u0026rdquo; and having a plan in general. Often people who start out with such practices find it too rigid, a bit too suffocating, as though all these rules and time-chunks have begun to take control of their life. That\u0026rsquo;s precisely where one can bring in mindfulness, by paying more attention to the present, to the task at hand. This allows us to use our plans to guide us, not control us. We don\u0026rsquo;t beat ourselves up when we\u0026rsquo;re mindfully doing a task, because we know its important. We\u0026rsquo;ve conciously decided to prioritize it.\nThe second connected concept to Mindfulness has to be Deliberate Practice or Deep Work. It is certainly a personal favourite of mine, ever since it has added more value to my days. At the heart of it, the idea in deep work is to push your cognitive levels to greater extents by paying more attention to your task, and focussing on just one task for long periods of time. It asks you to prioritize tasks, albeit in a very different way. What tasks among those you do routinely are those which value you the most? Where you are probably the best person to do it and generate more value through this tasks? Deep Work then asks you to relegate other logisitcal and mundane efforts, termed as \u0026ldquo;shallow work\u0026rdquo;, down your importance list. Instead, tackle your \u0026ldquo;deep tasks\u0026rdquo; first thing in the morning, or whichever part of the day you typically bring in your A-game. See the jarring similarities between the two?\n   Mindfulness doesn\u0026rsquo;t stop at being just a productive hack, its implications go well beyond, and plays a crucial role in other aspects of daily life. For instance, communication is one area which benefits from mindfulness. Regardless of whether you\u0026rsquo;re trying to pitch to a potential investor, persuade an unconvinced audience or just be a better listener with your social circle, mindfulness can help a lot. Next time when you listen to someone else speaking, listen without any judgement or bias. Listen without listening for the sake of just hearing the other person out. Especially when we have a script in our minds filled with what we would say next, and we are going to speak those lines, regardless of what the other person says. Sort of like a two-way scripted speech. Instead listen with purpose. Try to understand what they want to communicate, put yourself in their shoes and try to empathise with their situation. You\u0026rsquo;ll probably find yourself becoming someone people love to talk to.\nFor me, this explanation made me recall some of my friends who always come across as great listeners. Now, they aren\u0026rsquo;t necessarily reticent, in fact most of them are pretty active speakers too. However, everytime you speak to them, you would feel as though they have completely understood you. Somehow, you\u0026rsquo;re no longer the lone subject. All these people have one thing in common: they actively listen. Mindful communication was a really great take-home from this module.\nI was also able to connect Mindfulness to other modules taught in the course, one of them being social ettiequte. Being aware of others, their feelings and state of mind can certainly help push our social skills to the next level. It just makes some of the \u0026ldquo;good practices\u0026rdquo; listed under social ettiequte, which if you haven\u0026rsquo;t done them before, might seem too Victorian-era gentlelemanly for you. Holding out the door for the person in front of you, handing a person a glass of water or not interrupting people when they speak to you, can all come naturally when you actively observe and stay in the present.\nReading Suggestions:\n A very comprehensive evaluation of mindfulness applied to teams and leadership roles: Ellen Langer\u0026rsquo;s Mindfulness in the Age of Complexity, Harvard Business Review, 2014. A short Harvard Business Review (HBR) article on the balance between exploration and exploitation in engineering problems : How Mindfulness Can Help Engineers Solve Problems. You can couple mindfulness with meditation, which is a common way to introduce the concept: Getting Started with Mindfulness.  Understanding your Personality In this module, we were exposed to the general classification of personality traits. We started off with the rather well-known Myers Briggs Type Indicator (MBTI). An introspective self-report questionnaire based on the conceptual theory proposed by Carl Jung and Briggs Myers, who speculated that people experience the world using four principal psychological functions – sensation, intuition, feeling, and thinking – and that one of these four functions is dominant for a person most of the time. These give rise to four categories: Introversion/Extraversion, Sensing/Intuition, Thinking/Feeling, Judging/Perception, producing a total of 16 different types.\nThe first category deals with how a person draws his or her energy: whether it is through socialising and the external world (extraversion, E) or in isolation from their internal world (introversion, I). The second, Sensing-Intuition (S-N), deals with the way people perceive information, whether they rely more on external or internal stimuli for receiving this information. Thinking and Feeling (T-F) extends this to processing information, whether you are more of a logic based person or allow emotion to play a dominant role in decisions. Finally, Judging-Perceiving (J-P) .\n   Turns out, its not that simple\u0026hellip;   One can take the MBTI test online, by answering over two dozens of questions. So I did, besides being joined by my enthusiasitc wingmates (who knew personality tests were this exciting?) , and was classified ENTJ. The test makes a structured assessment of your responses based on various scenarios, judged objectively and not based on the context. Throughout the theory, one might notice two main dichotomies: \u0026ldquo;rational\u0026rdquo; (thinking, feeling) and \u0026ldquo;irrational\u0026rdquo; (senstion, intuition), which originates from Jung\u0026rsquo;s classification of cognitive functions.\nA certain way to look at the utility of this taxonomy is the way we have orientation for our hands. In the same way that writing with the left hand is difficult for a right-hander, so people tend to find using their opposite psychological preferences more difficult, though they can become more proficient (and therefore behaviorally flexible) with practice and development. One can also use this to play to their strengths, by grouping compatible people together. This is the most common way industry experts and HR professionals tend to use the MBTI. Again, it should be mentioned that these categories are not used to determine what\u0026rsquo;s \u0026ldquo;right\u0026rdquo; or what\u0026rsquo;s \u0026ldquo;wrong\u0026rdquo;. The Myers \u0026amp; Briggs Foundation goes a step further and urges people to view this as diversity, to take the test with confidentiality, be their best judge (questionnaires can tend to be incorrect, and is just an indication) and be given detailled feedback when they take these tests.\nUnfortunately, as elegant as the MBTI theory sounds, it tends to paint a black and white picture for many grey areas. As such despite having many pyschological based research around it, the scientific community at large has come to recognise this as a pseudoscience. In a popular (reddit?) joke, astrology is more accurate than the Myers-Briggs since it gets it wrong only 92% of the time, as compared to 94% inaccuracy of the personality categories. The major qualms with the method is its lack of a sound scientific basis. The assertion that dichotomies exist seem to be vaccuous, as many studies have found a normal distribution over the categories, instead of the assumed bimodal distribution. In other words, people don\u0026rsquo;t really have much of a preference, and might be as introverted as they are extroverted.\nA far more acceptable scale chart is the Big Five personality taxonomy. Also known as the five-factor model (FFM), this theory relates usage of langauge to personality traits. For example, someone described as conscientious is more likely to be described as \u0026ldquo;always prepared\u0026rdquo; rather than \u0026ldquo;messy\u0026rdquo;. The five factors are: openness to experience, conscientiousness, extraversion, agreeableness and neuroticism, giving rise to acronym OCEAN. The causality behind these traits is attributed to both genetics and upbringing, roughly evenly. Some of these traits such as conscientiousness, extraversion, openness to experience, and neuroticism tend to be relatively stable from childhood through adulthood.\n   Big Five Personality Chart   Openness to experience here refers to the general appreciation for something new in one\u0026rsquo;s life, be it adventure, art or emotion. It can be associated with dynamic thinking, unpredictability (maybe not fickleness) and overall being explorative. Conversely, those who are less open-minded tend to play it \u0026ldquo;safe\u0026rdquo; and stick to perseverance and a bit of dogmatisim. Conscientiousness refers to how people control their behavioural tendencies: whether they are more of self-disciplinarians or free spirits, how much one prefers planning over spontaneity. Extraversion is similar to the MBTI regime, but recognises that individuals might be a combination of the two. Agreeableness reflects individual differences in general concern for social harmony. Agreeable individuals value getting along with others, and disagreeable individuals place self-interest above getting along with others. This more or less positively correlates with one\u0026rsquo;s social relationships and negatively with military or stern leadership. Finally, neuroticism is the tendency to experience negative emotions, such as anger, anxiety, or depression. Often termed as emotional instability, it tests our emotional reactiveness to stressful scenarios. The Big Five test has also been assessed on some animal species, such as chimpanzees, with concepts largely carrying forward.\nOverall, this module gave me a valuable insight into human behavioural tendencies, even if I had to take it with a pinch of salt in certain contexts. Certainly, being concious of such flaws and limitations in these methods helps us appreciate the diversity each one of us brings to the table.\nReading Suggestions:\n As always, this is one of those topics where wikipedia provides a great introduction: Myers-Briggs Type Indicator. An online Big Five personality test: Truity FFM. A very balanced, textbook-ish approach to personality theories: The Personality Puzzle by David Funder.  ","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"860e6aed87717493d18ca28cb1123716","permalink":"https://varun19299.github.io/post/soft-skills/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/post/soft-skills/","section":"post","summary":"A blog post describing various modules covered so far in **MS4010:Soft Skills**","tags":["Journal"],"title":"Softskills Journal","type":"post"},{"authors":["V Sundar"],"categories":null,"content":" Note: This work was done as a part of the course project under the guidance of Prof. LA Prashanth for CS6700, IIT Madras. We are presently working on finite time analysis of this algorithm for both Regret Minimisation and Best Arm Identification settings.\nThe MRAS Algorithm Adapting to the Bandit Setting References  Yue et al., A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving, CVPR 2018. Carla docs.  ","date":1557878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"3f86343fe1f8659206c226b8b383d9a3","permalink":"https://varun19299.github.io/research/mras/","publishdate":"2019-05-15T00:00:00Z","relpermalink":"/research/mras/","section":"research","summary":"We propose a new multi-armed bandit algorithm called MRAS Bandits and show its utility for both Regret Minimisation and Best Arm Identification Settings. This work was done as a part of the course project for CS6700, IIT Madras.","tags":["Project"],"title":"MRAS Bandits: From Control Theory to Slot Machines","type":"research"},{"authors":["V Sundar"],"categories":null,"content":" V Sundar, S Siddique, K Mitra, “A Unified Framework for Lensless Image Recovery, to be submitted in the IEEE Transactions on Computational Imaging 2020.\n The 9th HULT Prize, Singapore. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.\n V Sundar, A Sadhu, R Nevatia, “FARCNN: Decoupling attributes from object detection”, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.\n Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.\n Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.\n Blog Post on extending LSD SLAM.\n Centre for Innovation, IIT Madras.\n Nick Antipa, Patrick Oare, Emrah Bostan, Ren Ng and Laura Waller. Video from Stills: Lensless Imaging with Rolling Shutter. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.\n  ","date":1557878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"518dae246c1364d9280733d8416f4743","permalink":"https://varun19299.github.io/sop-references-berkeley/","publishdate":"2019-05-15T00:00:00Z","relpermalink":"/sop-references-berkeley/","section":"","summary":"References used on my statement of purpose for UC Berkeley.","tags":["SOP References"],"title":"References: Statement of Purpose","type":"page"},{"authors":["V Sundar"],"categories":null,"content":" V Sundar, S Siddique, K Mitra, “A Unified Framework for Lensless Image Recovery”, to be submitted in the IEEE Transactions on Computational Imaging 2020.\n The 9th HULT Prize, Singapore. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.\n V Sundar, A Sadhu, R Nevatia, “FARCNN: Decoupling attributes from object detection”, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.\n Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.\n Julie Chang and Gordon Wetzstein - Deep Optics for Monocular Depth Estimation and 3D Object Detection. In IEEE International Conference on Computer Vision (ICCV), 2019.\n Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.\n Blog Post on extending LSD SLAM.\n Centre for Innovation, IIT Madras.\n  ","date":1557878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"2ea9c73b30d66a54708b6f4b908aa851","permalink":"https://varun19299.github.io/sop-references-stanford/","publishdate":"2019-05-15T00:00:00Z","relpermalink":"/sop-references-stanford/","section":"","summary":"References used on my statement of purpose for Stanford MS EE.","tags":["SOP References"],"title":"References: Statement of Purpose","type":"page"},{"authors":["V Sundar"],"categories":null,"content":" V Sundar, S Siddique, K Mitra, “A Unified Framework for Lensless Image Recovery”, to be submitted in the IEEE Transactions on Computational Imaging 2020.\n The 9th HULT Prize, Singapore. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.\n V Sundar, A Sadhu, R Nevatia, “FARCNN: Decoupling attributes from object detection”, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.\n Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.\n Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.\n Blog Post on extending LSD SLAM.\n Centre for Innovation, IIT Madras.\n  ","date":1557878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"9f5a57b27d94a100ae875242f4576447","permalink":"https://varun19299.github.io/sop-references/","publishdate":"2019-05-15T00:00:00Z","relpermalink":"/sop-references/","section":"","summary":"References used on my statement of purpose","tags":["SOP References"],"title":"References: Statement of Purpose","type":"page"},{"authors":["V Sundar","A Gokhale","N Krishna"],"categories":null,"content":" Note: This work was done as a part of the MBRDI Hackathon 2019, Bangalore. We placed third in the contest with our entry. For further details you can refer to our technical report or presentations.\nLiDAR point clouds are crucial in achieving level 4 and level 5 autonomous navigation. At the same time, obtaining LiDAR data for extensive scenes can be quite challenging. Hence, generating LiDAR data under synthetic simulations with control on parameters such as reflectivity, range, Field of View (FOV), etc would be useful. (See this if you would like to see a how real LiDAR outputs look like.)\nIn this project, we demonstrate an approach based on treating a camera as the equivalent of a LiDAR. In fact, this approach has been used elsewhere in computational photography and photogrammetry (or geometric computer vision): a very similar concept is prevelant in the very construction of a Time of Flight camera.\nA careful reader at this point would interrupt us to ask a natural question. Why not consider ray-casting, which is offered by many physics and game engines alike? Why then should someone choose to resort to such a camera based approach? With all the existing work behind parallel compute for ray casting, it seems like the perfect candidate for the job. The picture at the top represents this, and is in fact from the simulation engine Carla. However, the team at MBRDI wanted us to generate point clouds for a given road layout and for specified objects only.\nOpenDRIVE Layout The input data given is in the form of an OpenDRIVE layout, which essentially specifies road geometry and other objects (crossings, potential obstacles, poles, etc.). It does not include information on road texture or renderings of objects present. In the below picture, we show a sample Opendrive file.\n   Sample OpenDRIVE layout.   Note the same file rendered with software such as Sketchup 3D.\n   Same OpenDRIVE layout rendered by Sketch3D.   So, we wish to create a blackbox when given this road layout (and optionally some knowledge regarding objects on the road) can generate a 3D point cloud for specified targets, closely mimicking a real LiDAR.\n   Intended process flowchart.   Image-LIDAR Calibration Our first step is to draw correspondence between a given pixel and a 3D world point. This is done in a straightforward manner using projective geometry.\n   Proposed calibration scheme between a LiDAR and a camera. Credits: Yue et al., CVPR\u0026rsquo;18.   The goal of the calibration process is to find a function between each LiDAR Point and a pixel in the image. By utilising a depth map, we are able to do the converse too. The method we have adopted does the calibration automatically based on camera and LiDAR scanner parameters. It is similar to the camera perspective projection model once we set the camera and LiDAR at the same position, as shown in the above figure. Our method is based on work by Yue et al., CVPR 2018. We utilise their calibration scheme for our task as well.\nThe problem is formulated as follows: for a certain laser ray with azimuth angle $\\phi$ and zenith angle $\\theta$, calculate the index $(i,j)$ of the corresponding pixel on image. ${F_c},{F_o},{P}, {P^{\\prime}}$ and $P_{far}$ are 3-D coordinates of a) centre of camera/LiDAR scanner, b) centre of camera near clipping plane. c) point first hit by the virtual laser ray (in red), d) pixel on image corresponding to $P$, e) a point far away in the laser direction, respectively. Also, $m$ and $n$ are the width and height of the near clipping plane. $\\gamma$ is $1/2$ vertical FOV of camera while $\\psi$ is $1/2$ vertical FOV of the LiDAR scanner. Note that LiDAR scanner FOV is usually smaller than camera FOV, since there is usually no object in the top part of the image, and the emitting laser to open space is not necessary.\nAfter a series of 3D geometry calculation, we can get: $$i = \\frac{R_m}{m} (f\\cdot tan\\gamma \\cdot \\frac{m}{n} - \\frac{f}{cos\\theta} \\cdot tan\\phi)$$\n$$j = \\frac{R_n}{n} (f\\cdot tan\\gamma + f \\cdot tan\\theta)$$\nwhere $f = \\||\\vec{F_c F_o}||$, and $(R_m,R_n)$ is the pixel resolution of the image/near clipping plane. Further, in order for the ray casting API to work properly, the 3D coordinates of $P_{far}$ are also required. Using similar 3D geometry calculations, we obtain:\n$$P^{\\prime} = F_c + f \\cdot \\vec{x_c} - \\frac{f}{cos\\theta} \\cdot tan \\phi \\cdot \\vec{y_c} -f\\cdot tan\\theta \\cdot \\vec{z_c}$$\n$$P_{far}= F_c + k \\cdot (P^{\\prime}- F_c)$$\nwhere k is a large coefficient, and $\\vec{x_c}, \\vec{y_c}, \\vec{z_c}$ are unit vectors of the camera axis in the world coordinate system. After simulation, both image and point cloud of the specified in-model scene are collected by the framework.\nWith this step, we can associate each 3D point to a pixel and each (valid) pixel to a given 3D point according to the LiDAR\u0026rsquo;s construction.\nIdentify Targets, Introduce Sparsity The second step involves introducing artefacts such as sparsity and intensity variation typically found in real LiDARs such as a Velodyne VLP-16. We can either do this by modelling the physics for reflectivity (with models of varying complexity) or by using data as a prior. Since our purpose here is to just demonstrate a proof of concept, we take the easier way out and turn to data-driven priors. In essence, we utilise a deep network which converts image taken by a camera to a sparse intensity map akin to a LiDAR.\n   Sample sparse depth points from the KITTI Dataset.   For the required data, we turn to the popular KITTI dataset, which includes VLP-16 acquisitions as shown below. For a visualisation of LiDAR data in the KITTI dataset, take a look at this excellant blog post. A sample data point is shown in the figure above. In this step, we also choose to identify targets of interest according to the given road layout (which is in the OpenDrive format).\nReproject Points back to 3D Finally, having selected our objects of interest in the camera frame, we can reproject back to 3D. We select these objects using routine tools from computer vision such as segmentation, contour detection, object detection etc. In order to obtain intensities at those points, we simply follow a naive inverse-square power law model. Here\u0026rsquo;s a sample point cloud rendering.\n   Sample point cloud.   Concluding Notes Our solution to this hackathon conducted by MBRDI involved a simple bijection between LiDARs and cameras in the simulation world. With this correspondence, we are oepn to a lot of tools from the image processing and computer vision community. We\u0026rsquo;ve certainly been sub-optimal in a lot of places, including:\n Simple assumptions on intensity variation. Not handling multiple or specular reflections. Point clouds rendered is with respect to texture offered by such game engines.  We shall try addressing some of these as future work.\nReferences  Yue et al., A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving, CVPR 2018. Carla docs.  ","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"5c23c77c0bf2abd144bf41fcdf227c1b","permalink":"https://varun19299.github.io/projects/lidar/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/projects/lidar/","section":"projects","summary":"Our entry at MBRDI Hackathon 2019, describing a correspondence between cameras and LiDARs for synthetic simulation. We utilised the autnomous car simulator CARLA extensively throughout this process.","tags":["Project"],"title":"Simulating Commercial LiDARs using Physics based Game Engines","type":"projects"},{"authors":null,"categories":null,"content":"Here\u0026rsquo;s a bunch of courses I\u0026rsquo;ve taken as an undergrad at IIT-Madras:\n AI or Computer Vision Courses:  Computational Photography Advances in the theory of Deep Learning Multi-Armed Bandits Reinforcement Learning Deep Learning for Computer Vision Simultaneous Localisation and Mapping (SLAM, self study) GPU Programming (audited) Computer Vision (audited)  General Programming Courses:  Applied Programming Lab Data Structures and Algorithms Introduction to Programming  Core Electrical Courses:  Analog circuits Analog devices Solid State Devices Microprocessors Control Theory Electrical Machines Signals and Systems; Discrete Signal Processing  Math Courses:  Estimation Theory Convex Optimisation Complex Analysis Probability Theory Multivariable Calculus  Other Science Electives:  Physics 1010, 1020 (Mechanics and Electromagnetics)  Humanities and Other Electives:  Soft Skills German 1   Grade transcript available on request.\n","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576048895,"objectID":"cbbb9a333220e2d9aaa04e916be12c4f","permalink":"https://varun19299.github.io/education/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/education/","section":"","summary":"Here we describe how to add a page to your site.","tags":null,"title":"Education","type":"page"},{"authors":["V Sundar"],"categories":null,"content":" V Sundar, S Siddique, K Mitra, “A Unified Framework for Lensless Image Recovery, to be submitted in the IEEE Transactions on Computational Imaging 2020.\n The 9th HULT Prize, Singapore. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.\n V Sundar, A Sadhu, R Nevatia, “FARCNN: Decoupling attributes from object detection”, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.\n Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3d — Learning phase masks for passive single view depth estimation. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.\n Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3d: High-quality depth sensing with polarization cues. In International Conference on Computer Vision (ICCV), 2015.\n Blog Post on extending LSD SLAM.\n Centre for Innovation, IIT Madras.\n Katherine L. Bouman, Vickie Ye, Adam B. Yedidia, Fre ́do Durand, Gregory W. Wornell, Antonio Torralba and William T. Freeman. Turning Corners into Cameras: Principles and Methods. In International Conference on Computer Vision (ICCV), 2017.\n Tianfan Xue, Jiajun Wu, Katherine L. Bouman and William T. Freeman. Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks. In Advances In Neural Information Processing Systems (NeurIPS) 2016.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e6c8483b1f299e7faf80f74578724c6","permalink":"https://varun19299.github.io/sop-references-caltech/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/sop-references-caltech/","section":"","summary":"References used on my statement of purpose for UC Berkeley.","tags":["SOP References"],"title":"References: Statement of Purpose","type":"page"}]