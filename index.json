[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a senior undergraduate in the Department of Electrical Engineering at the Indian Institute of Technology Madras (IIT-M), India. My goal is to pursue a PhD in the intersection of computer vision and computational photography, begining Fall 2020.\nThis website is a collection of my research projects and other exploratory projects done. There is also a solitary blog-post regarding a pet research idea of mine: CNN-SLAM. Hopefully, there\u0026rsquo;ll be a few others joining this one soon!\nIn case any of these topics piques your interests and you would like to talk about them (or just say hi) feel free to contact me!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1570379802,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://varun19299.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a senior undergraduate in the Department of Electrical Engineering at the Indian Institute of Technology Madras (IIT-M), India. My goal is to pursue a PhD in the intersection of computer vision and computational photography, begining Fall 2020.\nThis website is a collection of my research projects and other exploratory projects done. There is also a solitary blog-post regarding a pet research idea of mine: CNN-SLAM. Hopefully, there\u0026rsquo;ll be a few others joining this one soon!","tags":null,"title":"Varun Sundar","type":"authors"},{"authors":null,"categories":null,"content":"You can write to me at [varun19299] AT [gmail.com]. I\u0026rsquo;m not very active on social media, besides the occasional glance at twitter or reddit. My twitter handle is @varun19299.\n","date":1570320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570320000,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://varun19299.github.io/contact/","publishdate":"2019-10-06T00:00:00Z","relpermalink":"/contact/","section":"","summary":"You can write to me at [varun19299] AT [gmail.com]. I\u0026rsquo;m not very active on social media, besides the occasional glance at twitter or reddit. My twitter handle is @varun19299.","tags":null,"title":"Contact Me","type":"page"},{"authors":null,"categories":null,"content":" Note: This blog post is meant to serve as a journal or a log of sorts of my learnings and reflections in a course I took up this semester (Fall 2019). It is not meant to be a reliable course transcript and as such might skim on some portions, while emphasising on others.\nModules covered under the course (to date):\n Search for purpose: Aug 1st, 2nd week Mindfulness and the power of habits _ Understanding your personality Decision making and critical thinking Effective communication  Search for Purpose In this introductory module, we began by discussing our expectations and aspirations from the course.\nIkigai is the Japanese concept of finidng a purpose in life. The philosphy revolves around the idea that once you discover your true, unwaivering purpose in life, the journey called life becomes much more meaningful. This practice historically originates from the\nAt the onset, this did not make much sense to me. Yes, goal making is important and is quite often stressed in many work-ethic related workshops. But there\u0026rsquo;s one caveat to goal making: finding the right goal, especially since it is not trivial to understand what is relevant. Luckily, the ikigai addresses this too,\nReading Suggestions:\nMindfulness Time Management\nPersonal notes\nUnderstanding your Personality Group Presentations Social Competence and Group Dynamics Emotional Competence Keeping your cool Role of healthy activities and sleep. Emotional Insulation. Allows for criticism to tolerable limits, such a way that it doesnot have dertrimental effects. Components and tools include ego\n","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570379802,"objectID":"cbcefafb0d5035ddc624f380789f7d35","permalink":"https://varun19299.github.io/post/soft-skills/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/post/soft-skills/","section":"post","summary":"A blog post describing various modules covered so far in \"MS4010:Soft Skills\"","tags":["Journal"],"title":"Softskills Journal","type":"post"},{"authors":["V Sundar","A Gokhale","N Krishna"],"categories":null,"content":" Note: This work was done as a part of the MBRDI Hackathon 2019, Bangalore. We placed third in the contest with our entry. For further details you can refer to our technical report or presentations.\nLiDAR point clouds are crucial in achieving level 4 and level 5 autonomous navigation. At the same time, obtaining LiDAR data for extensive scenes can be quite challenging. Hence, generating LiDAR data under synthetic simulations with control on parameters such as reflectivity, range, Field of View (FOV), etc would be useful. (See this if you would like to see a how real LiDAR outputs look like.)\nIn this project, we demonstrate an approach based on treating a camera as the equivalent of a LiDAR. In fact, this approach has been used elsewhere in computational photography and photogrammetry (or geometric computer vision): a very similar concept is prevelant in the very construction of a Time of Flight camera.\nA carefull reader at this point would interrupt us to ask a natural question. Why not consider ray-casting, which is offered by many physics and game engines alike? Why then should someone choose to resort to such a camera based approach? With all the existing work behind parallel compute for ray casting, it seems like the perfect candidate for the job. The picture at the top represents this, and is in fact from the simulation engine Carla. However, the team at MBRDI wanted us to generate point clouds for a given road layout and for selected objects only.\nOpenDRIVE Layout The input data given is in the form of OpenDRIVE layout, which essentially specifies road geometry and other objects (crossings, potential obstacles, poles, etc.). It does not include information on road texture or renderings of objects present. In the below picture, we show a sample Opendrive file.\n   Sample OpenDRIVE layout.   Note the same file rendered with software such as Sketchup 3D.\n   Same OpenDRIVE layout rendered by Sketch3D.   So, we wish to create a blackbox when given this road layout (and optionally some knowledge regarding objects on the road) can generate a 3D point cloud for specified targets, closely mimicking a real LiDAR.\n   Intended process flowchart.   Image-LIDAR Calibration Our first step is to draw correspondence between a given pixel and a 3D world point. This is done in a straightforward manner using projective geometry.\n   Proposed calibration scheme between a LiDAR and a camera. Credits: Yue et al., CVPR\u0026rsquo;18.   The goal of the calibration process is to find a function between each LiDAR Point and a pixel in the image. By utilising a depth map, we are able to do the converse too. The method we have adopted does the calibration automatically based on camera and LiDAR scanner parameters. It is similar to the camera perspective projection model once we set the camera and LiDAR at the same position, as shown in the above figure. Our method is based on work by Yue et al., CVPR 2018. We utilise their calibration scheme for our task as well.\nThe problem is formulated as follows: for a certain laser ray with azimuth angle $\\phi$ and zenith angle $\\theta$, calculate the index $(i,j)$ of the corresponding pixel on image. ${F_c},{F_o},{P}, {P^{\\prime}}$ and $P_{far}$ are 3-D coordinates of a) centre of camera/LiDAR scanner, b) centre of camera near clipping plane. c) point first hit by the virtual laser ray (in red), d) pixel on image corresponding to $P$, e) a point far away in the laser direction, respectively. Also, $m$ and $n$ are the width and height of the near clipping plane. $\\gamma$ is $1/2$ vertical FOV of camera while $\\psi$ is $1/2$ vertical FOV of the LiDAR scanner. Note that LiDAR scanner FOV is usually smaller than camera FOV, since there is usually no object in the top part of the image, and the emitting laser to open space is not necessary.\nAfter a series of 3D geometry calculation, we can get: $$i = \\frac{R_m}{m} (f\\cdot tan\\gamma \\cdot \\frac{m}{n} - \\frac{f}{cos\\theta} \\cdot tan\\phi)$$\n$$j = \\frac{R_n}{n} (f\\cdot tan\\gamma + f \\cdot tan\\theta)$$\nwhere $f = \\||\\vec{F_c F_o}||$, and $(R_m,R_n)$ is the pixel resolution of the image/near clipping plane. Further, in order for the ray casting API to work properly, the 3D coordinates of $P_{far}$ are also required. Using similar 3D geometry calculations, we obtain:\n$$P^{\\prime} = F_c + f \\cdot \\vec{x_c} - \\frac{f}{cos\\theta} \\cdot tan \\phi \\cdot \\vec{y_c} -f\\cdot tan\\theta \\cdot \\vec{z_c}$$\n$$P_{far}= F_c + k \\cdot (P^{\\prime}- F_c)$$\nwhere k is a large coefficient, and $\\vec{x_c}, \\vec{y_c}, \\vec{z_c}$ are unit vectors of the camera axis in the world coordinate system. After simulation, both image and point cloud of the specified in-model scene are collected by the framework.\nWith this step, we can associate each 3D point to a pixel and each (valid) pixel to a given 3D point according to the LiDAR\u0026rsquo;s construction.\nIdentify Targets, Introduce Sparsity The second step involves introducing artefacts such as sparsity and intensity variation typically found in real LiDARs such as a Velodyne VLP-16. We can either do this by modelling the physics for reflectivity (with models of varying complexity) or by using data as a prior. Since our purpose here is to just demonstrate a proof of concept, we take the easier way out and turn to data-driven priors. In essence, we utilise a deep network which converts image taken by a camera to a sparse intensity map akin to a LiDAR.\n   Sample sparse depth points from the KITTI Dataset.   For the required data, we turn to the popular KITTI dataset, which includes VLP-16 acquisitions as shown below. For a visualisation of LiDAR data in the KITTI dataset, take a look at this excellant blog post. A sample data point is shown in the figure above. In this step, we also choose to identify targets of interest according to the given road layout (which is in the OpenDrive format).\nReproject Points back to 3D Finally, having selected our objects of interest in the camera frame, we can reproject back to 3D. We select these objects using routine tools from computer vision such as segmentation, contour detection, object detection etc. In order to obtain intensities at those points, we simply follow a naive inverse-square power law model. Here\u0026rsquo;s a sample point cloud rendering.\n   Sample point cloud.   Concluding Notes Our solution to this hackathon conducted by MBRDI involved a simple bijection between LiDARs and cameras in the simulation world. With this correspondence, we are oepn to a lot of tools from the image processing and computer vision community. We\u0026rsquo;ve certainly been sub-optimal in a lot of places, including:\n Simple assumptions of intensity variation Not handling multiple reflections  We shall try addressing some of these as future work.\nReferences  Yue et al., A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving, CVPR 2018. Carla docs.  ","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548460800,"objectID":"5c23c77c0bf2abd144bf41fcdf227c1b","permalink":"https://varun19299.github.io/projects/lidar/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/projects/lidar/","section":"projects","summary":"Our entry at MBRDI Hackathon 2019, describing a correspondence between cameras and LiDARs for synthetic simulation. We utilised the autnomous car simulator CARLA extensively throughout this process.","tags":["Project"],"title":"Simulating Commercial LiDARs using Physics based Game Engines","type":"projects"},{"authors":null,"categories":null,"content":"Here\u0026rsquo;s a bunch of courses I\u0026rsquo;ve taken as an undergrad at IIT-Madras:\n AI or Computer Vision Courses:  Computational Photography Advances in the theory of Deep Learning Multi-Armed Bandits Reinforcement Learning Deep Learning for Computer Vision Simultaneous Localisation and Mapping (SLAM, self study) GPU Programming (audited) Computer Vision (audited)  General Programming Courses:  Applied Programming Lab Data Structures and Algorithms Introduction to Programming  Core Electrical Courses:  Analog circuits Analog devices Solid State Devices Microprocessors Control Theory Electrical Machines Signals and Systems; Discrete Signal Processing  Math Courses:  Estimation Theory Convex Optimisation Complex Analysis Probability Theory Multivariable Calculus  Other Science Electives:  Physics 1010, 1020 (Mechanics and Electromagnetics)  Humanities and Other Electives:  Soft Skills German 1   Grade transcript available on request.\n","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570379802,"objectID":"cbbb9a333220e2d9aaa04e916be12c4f","permalink":"https://varun19299.github.io/education/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/education/","section":"","summary":"Here we describe how to add a page to your site.","tags":null,"title":"Education","type":"page"}]