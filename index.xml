<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Varun Sundar</title>
    <link>https://varun19299.github.io/</link>
      <atom:link href="https://varun19299.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Varun Sundar</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://varun19299.github.io/img/icon-192.png</url>
      <title>Varun Sundar</title>
      <link>https://varun19299.github.io/</link>
    </image>
    
    <item>
      <title>Extending LSD SLAM with Computational Photography</title>
      <link>https://varun19299.github.io/post/extend-lsd-slam/</link>
      <pubDate>Thu, 28 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/post/extend-lsd-slam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Attribute Transfer in a GAN Framework</title>
      <link>https://varun19299.github.io/research/attribute-transfer/</link>
      <pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/research/attribute-transfer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contact Me</title>
      <link>https://varun19299.github.io/contact/</link>
      <pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/contact/</guid>
      <description>&lt;p&gt;You can write to me at &lt;strong&gt;[varun19299] AT [gmail.com]&lt;/strong&gt;. I&amp;rsquo;m not very active on social media, besides the occasional glance at twitter or reddit. My twitter handle is @varun19299.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Softskills Journal</title>
      <link>https://varun19299.github.io/post/soft-skills/</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/post/soft-skills/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This blog post is meant to serve as a journal or a log of sorts of my learnings and reflections in a course I took up this semester (Fall 2019). Technically, owing to its sparse insertions, it is barely a journal, maybe more of a reflection. And certainly no memoir! It is not meant to be a reliable course transcript and as such might skim on some portions, while emphasising on others.&lt;/p&gt;

&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#search-for-purpose&#34;&gt;Search for Purpose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mindfulness&#34;&gt;Mindfulness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#understanding-your-personality&#34;&gt;Understanding your Personality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;search-for-purpose&#34;&gt;Search for Purpose&lt;/h2&gt;

&lt;p&gt;In this introductory module, we began by discussing our expectations and aspirations from the course. For me, I wanted to use this course as a much needed opportunity for introspection, perhaps meet a few new people (&lt;em&gt;at insti, you can never  have met everyone!&lt;/em&gt;) and may be some wise take-home advice would be an added bonus. &lt;em&gt;&lt;strong&gt;Edit, 19th November, 2019:&lt;/strong&gt; After spending a semester in the course, I can say I received a lot more than my initial objectives, and I&amp;rsquo;m glad about it.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We began with the goal of visioning with a little help from a certain Eastern concept.  &lt;strong&gt;Ikigai&lt;/strong&gt;, roughly translating to &amp;ldquo;thing that you live for&amp;rdquo;, is the Japanese concept of finidng a purpose in life. The philosphy revolves around the idea that once you discover your true, unwaivering purpose in life, the journey called life becomes much more meaningful. This philosophy historically originates as a ritualisitc practice from the southern most prefecture of Japan, &lt;a href=&#34;https://en.wikipedia.org/wiki/Okinawa_Prefecture&#34; target=&#34;_blank&#34;&gt;Okinawa&lt;/a&gt; on the islands of Ryuku. For long, the inhabitants of the island  have been quentissentially associated with the lack of any desire to retire. They continue, as long as their physical bodies permit,  doing their favourtie activities. In fact they wake up everyday with the desire to perform that activity. Which is quite ironic compared to our modern lives, where people as early as their 30&amp;rsquo;s start thinking about retirement!&lt;/p&gt;

&lt;p&gt;In a famous TED Talk (inset below), National Geographic reporter &lt;a href=&#34;https://en.wikipedia.org/wiki/Dan_Buettner&#34; target=&#34;_blank&#34;&gt;Dan Buettner&lt;/a&gt; posited this existence of a definite reason to be the cause for the long lifespans of the island&amp;rsquo;s inhabitants. This sort of feels intuitive to me (and hopefully to a lot of others too!). Having a purpose, something you wake up to each day, can be pretty satisfying. And that often translates to happiness and general well-being. In the course, taking inspiration from the same, we set out on a small exercise to find our &amp;ldquo;purpose&amp;rdquo; in life.&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/ff40YiMmVkU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;At the onset, this did not make much sense to me. Yes, goal making is important and is quite often stressed in many work-ethic related workshops. But there&amp;rsquo;s one caveat to goal making: finding the right goal, especially since it is not trivial to understand what is relevant. Luckily, the ikigai addresses this too, by asking four insightful questions. It asks first what you love to do, something which gives you joy of some form or doesn&amp;rsquo;t feel like a burden when you do it. Second, what is it that you do well. This could include activities in which you are skilled or talented. You may or may not derive satisfaction out of this. Intersecting these two question should give you a list of things you&amp;rsquo;re passionate about.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://miro.medium.com/max/1124/1*cJk9deGJ2aizplaTslEXLA.jpeg&#34; &gt;
&lt;img src=&#34;https://miro.medium.com/max/1124/1*cJk9deGJ2aizplaTslEXLA.jpeg&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;Notice that this question isn&amp;rsquo;t concerned with how others perceive your tasks. Well, that&amp;rsquo;s the third. What are things you do that others value? This could be monetary value, intellectual value or even a non-materialistic value. But it might be easier for you to relate to this question if you rephrase it as: &amp;ldquo;what is it that people are willing to pay me for?&amp;rdquo;. Here, the intersection of the second and third questions give you a profession: something you are skilled at, and is valuable to others. The final question, and maybe the most important of them, is: what does the world require now? The answer to this question allows us to rise above just ourselves and contribute to the larger cause of humanity. For those of you who are fortunate to have activities at the intersection at questions one and four, voila, you have a mission.&lt;/p&gt;

&lt;p&gt;Finally, ikigai is the harmonious synergy of all these four questions. In other words it urges you to find an activity which could be a profession, passion, vocation and mission for you at the same time. Pretty confounding if you ask me. Now it might not be easy to find this sweet spot in one go. It might take quite a while actually. But the journey involved in finding your ikigai can be quite satisfying. It can also serve as a guiding light for rethinking some of your favourite activities. Say you like teaching and see it as a viable profession: maybe 5 or 10 years down the line. Now how would we transform this into something the world needs. Well it so happens that quality education is number 4 on the UN &lt;a href=&#34;https://en.wikipedia.org/wiki/Sustainable_Development_Goals&#34; target=&#34;_blank&#34;&gt;Sustainable Development Goals&lt;/a&gt; (SDGs) list. Off the top of my head, you could either work on making education more accessible to the global population - there are over 115 million without access to quality primary education - or work on effective communication and disttilation of knowledge (which in my opinion, is pretty important too, &lt;a href=&#34;https://distill.pub/2017/research-debt/&#34; target=&#34;_blank&#34;&gt;read this&lt;/a&gt; for a comprehensive article on how lack of distilling knowledge plagues schools and academia alike).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reading Suggestions:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;For an interesting take on finding meaningful work and whether &amp;ldquo;the one&amp;rdquo; exists (&lt;em&gt;no this is not relationship advice&lt;/em&gt;): &lt;a href=&#34;https://heleo.com/conversation-adam-smiley-poswolsky-on-how-to-find-meaningful-work-in-the-age-of-job-hopping/11431/&#34; target=&#34;_blank&#34;&gt;Adam “Smiley” Poswolsky on How to Find Meaningful Work in the Age of Job-Hopping&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If Forbes is your thing: &lt;a href=&#34;https://www.forbes.com/sites/bhaligill/2017/09/29/discover-your-passion-or-ikigai-with-4-simple-tips/#9d538c853f7b&#34; target=&#34;_blank&#34;&gt;Discover Your Passion &amp;ndash; Or &amp;lsquo;Ikigai&amp;rsquo; &amp;ndash; With 4 Simple Tips&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;An empirical study on the importance of theoretical work: &lt;a href=&#34;https://doi.org/10.1080/00048402.2018.1428636&#34; target=&#34;_blank&#34;&gt;Meaningful Work, by Andrea Veltman&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;mindfulness&#34;&gt;Mindfulness&lt;/h2&gt;

&lt;p&gt;Another module which deeply resonanted with me is that of mindfulness. My earliest exposure to the concept was when I picked up a book from Mom&amp;rsquo;s reading stack one afternoon and as luck would have it, I ran into the &lt;a href=&#34;https://www.goodreads.com/book/show/6708.The_Power_of_Now&#34; target=&#34;_blank&#34;&gt;Power of Now&lt;/a&gt; by &lt;a href=&#34;https://en.wikipedia.org/wiki/Eckhart_Tolle&#34; target=&#34;_blank&#34;&gt;Ekhard Tole&lt;/a&gt;. The book isn&amp;rsquo;t dreadful, but it certainly was heavy for a school kid. Yet, there were a few ideas from the book that I actually did like. The idea of turning off our minds completely for a while everyday and becoming passive observers of our thought process: two very calming rituals.&lt;/p&gt;

&lt;p&gt;Mindfulness is a more precise term for this broad idea of living more actively in the present, noticing the smaller details around us. It could  be as simple as stopping by to admire the fresh dew on a cold morning, or noticing how that stray cat does a weird movement with its ears everytime it gets up. But the idea is to observe, observe both passively and actively. Active in your senses, and passive so as to not let your biases and notions cloud the scene. If this sounds to you like a line &lt;em&gt;Oogway&lt;/em&gt; from &lt;em&gt;Kung Fu Panda&lt;/em&gt; would say, you&amp;rsquo;re probably on the right track. And since we brought up &lt;em&gt;Oogway&lt;/em&gt;, I think this line from the movie is pretty apt in this context:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Yesterday is history,
tomorrow is a mystery,
and today is a gift&amp;hellip;
that&amp;rsquo;s why they call it the present”&lt;/p&gt;
&lt;/blockquote&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://c7.uihere.com/files/320/946/351/oogway-master-shifu-po-tai-lung-youtube-youtube.jpg&#34; data-caption=&#34;Oogway spreading his wisdom, circa 2010.&#34;&gt;
&lt;img src=&#34;https://c7.uihere.com/files/320/946/351/oogway-master-shifu-po-tai-lung-youtube-youtube.jpg&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;400&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Oogway spreading his wisdom, circa 2010.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Although seemingly unconnected, I feel mindfulness shares a lot of concepts in common with other practices including time management and deliberate practice (or deep work). &lt;strong&gt;Time Management,&lt;/strong&gt; a concept, dreaded by procrastinators and capitalised by life-coaches, it certainly goes hand-in-hand with mindfulness. Most time-management practices involve to varying degrees the art of prioritizing, dividing your day into &amp;ldquo;phases&amp;rdquo; and having a plan in general. Often people who start out with such practices find it too rigid, a bit too suffocating, as though all these rules and time-chunks have begun to take control of their life. That&amp;rsquo;s precisely where one can bring in mindfulness, by paying more attention to the present, to the task at hand. This allows us to use our plans to guide us, not control us. We don&amp;rsquo;t beat ourselves up when we&amp;rsquo;re mindfully doing a task, because we know its important. We&amp;rsquo;ve conciously decided to prioritize it.&lt;/p&gt;

&lt;p&gt;The second connected concept to Mindfulness has to be Deliberate Practice or Deep Work. It is certainly a personal favourite of mine, ever since it has added more value to my days. At the heart of it, the idea in deep work is to push your cognitive levels to greater extents by paying more attention to your task, and focussing on just one task for long periods of time. It asks you to prioritize tasks, albeit in a very different way. What tasks among those you do routinely are those which value you the most? Where you are probably the best person to do it and generate more value through this tasks? Deep Work then asks you to relegate other logisitcal and mundane efforts, termed as &amp;ldquo;shallow work&amp;rdquo;, down your importance list. Instead, tackle your &amp;ldquo;deep tasks&amp;rdquo; first thing in the morning, or whichever part of the day you typically bring in your A-game. See the jarring similarities between the two?&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://images-na.ssl-images-amazon.com/images/I/418Mmq-lMLL._SX316_BO1,204,203,200_.jpg&#34; &gt;
&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/418Mmq-lMLL._SX316_BO1,204,203,200_.jpg&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;400&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;Mindfulness doesn&amp;rsquo;t stop at being just a productive hack, its implications go well beyond, and plays a crucial role in other aspects of daily life. For instance, communication is one area which benefits from mindfulness. Regardless of whether you&amp;rsquo;re trying to pitch to a potential investor, persuade an unconvinced audience or just be a better listener with your social circle, mindfulness can help a lot. Next time when you listen to someone else speaking, listen without any judgement or bias. Listen without listening for the sake of just hearing the other person out. Especially when we have a script in our minds filled with what we would say next, and we &lt;em&gt;are&lt;/em&gt;  going to speak those lines, regardless of what the other person says. Sort of like a two-way scripted speech. Instead listen with purpose. Try to understand what they want to communicate, put yourself in their shoes and try to empathise with their situation. You&amp;rsquo;ll probably find yourself becoming someone people love to talk to.&lt;/p&gt;

&lt;p&gt;For me, this explanation made me recall some of my friends who always come across as great listeners. Now, they aren&amp;rsquo;t necessarily reticent, in fact most of them are pretty active speakers too. However, everytime you speak to them, you would feel as though they have completely understood you. Somehow, you&amp;rsquo;re no longer the lone subject. All these people have one thing in common: they actively listen. Mindful communication was a really great take-home from this module.&lt;/p&gt;

&lt;p&gt;I was also able to connect Mindfulness to other modules taught in the course, one of them being social ettiequte. Being aware of others, their feelings and state of mind can certainly help push our social skills to the next level. It just makes some of the &amp;ldquo;good practices&amp;rdquo; listed under social ettiequte, which if you haven&amp;rsquo;t done them before, might seem too Victorian-era gentlelemanly for you. Holding out the door for the person in front of you, handing a person a glass of water or not interrupting people when they speak to you, can all come naturally when you actively observe and stay in the present.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reading Suggestions:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A very comprehensive evaluation of mindfulness applied to teams and leadership roles: &lt;a href=&#34;https://hbr.org/2014/03/mindfulness-in-the-age-of-complexity&#34; target=&#34;_blank&#34;&gt;Ellen Langer&amp;rsquo;s Mindfulness in the Age of Complexity, Harvard Business Review, 2014&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A short Harvard Business Review (HBR) article on the balance between exploration and exploitation in engineering problems : &lt;a href=&#34;https://hbr.org/2019/01/how-mindfulness-can-help-engineers-solve-problems&#34; target=&#34;_blank&#34;&gt;How Mindfulness Can Help Engineers Solve Problems&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You can couple mindfulness with meditation, which is a common way to introduce the concept: &lt;a href=&#34;https://www.mindful.org/meditation/mindfulness-getting-started/&#34; target=&#34;_blank&#34;&gt;Getting Started with Mindfulness&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;understanding-your-personality&#34;&gt;Understanding your Personality&lt;/h2&gt;

&lt;p&gt;In this module, we were exposed to the general classification of personality traits. We started off with the rather well-known Myers Briggs Type Indicator (MBTI). An introspective self-report questionnaire based on the conceptual theory proposed by Carl Jung and Briggs Myers, who speculated that people experience the world using four principal psychological functions – sensation, intuition, feeling, and thinking – and that one of these four functions is dominant for a person most of the time. These give rise to four categories:  Introversion/Extraversion, Sensing/Intuition, Thinking/Feeling, Judging/Perception, producing a total of 16 different types.&lt;/p&gt;

&lt;p&gt;The first category deals with how a person draws his or her energy: whether it is through socialising and the external world (extraversion, E) or in isolation from their internal world (introversion, I). The second, Sensing-Intuition (S-N), deals with the way people perceive information, whether they rely more on external or internal stimuli for receiving this information. Thinking and Feeling (T-F) extends this to processing information, whether you are more of a logic based person or allow emotion to play a dominant role in decisions. Finally, Judging-Perceiving (J-P) .&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://cdn.shopify.com/s/files/1/0100/5392/articles/Mouthpiece_VdayMeyersBriggs1.progressive.jpg?v=1571071731&#34; data-caption=&#34;Turns out, its not that simple&amp;hellip;&#34;&gt;
&lt;img src=&#34;https://cdn.shopify.com/s/files/1/0100/5392/articles/Mouthpiece_VdayMeyersBriggs1.progressive.jpg?v=1571071731&#34; alt=&#34;&#34; width=&#34;600&#34; height=&#34;600&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Turns out, its not that simple&amp;hellip;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;One can take the MBTI test &lt;a href=&#34;https://www.16personalities.com/free-personality-test&#34; target=&#34;_blank&#34;&gt;online&lt;/a&gt;, by answering over two dozens of questions. So I did, besides being joined by my enthusiasitc wingmates (who knew personality tests were &lt;em&gt;this&lt;/em&gt; exciting?) , and was classified ENTJ. The test makes a structured assessment of your responses based on various scenarios, judged objectively and not based on the context. Throughout the theory, one might notice two main dichotomies: &amp;ldquo;rational&amp;rdquo; (thinking, feeling) and &amp;ldquo;irrational&amp;rdquo; (senstion, intuition), which originates from Jung&amp;rsquo;s classification of cognitive functions.&lt;/p&gt;

&lt;p&gt;A certain way to look at the utility of this taxonomy is the way we have orientation for our hands. In the same way that writing with the left hand is difficult for a right-hander, so people tend to find using their opposite psychological preferences more difficult, though they can become more proficient (and therefore behaviorally flexible) with practice and development. One can also use this to play to their strengths, by grouping compatible people together. This is the most common way industry experts and HR professionals tend to use the MBTI. Again, it should be mentioned that these categories are not used to determine what&amp;rsquo;s &amp;ldquo;right&amp;rdquo; or what&amp;rsquo;s &amp;ldquo;wrong&amp;rdquo;. The Myers &amp;amp; Briggs Foundation goes a step further and urges people to view this as diversity, to take the test with confidentiality, be their best judge (questionnaires can tend to be incorrect, and is just an indication) and be given detailled feedback when they take these tests.&lt;/p&gt;

&lt;p&gt;Unfortunately, as elegant as the MBTI theory sounds, it tends to paint a black and white picture for many grey areas. As such despite having many pyschological based research around it, the scientific community at large has come to recognise this as a pseudoscience. In a popular (&lt;em&gt;reddit?&lt;/em&gt;) joke, astrology is more accurate than the Myers-Briggs since it gets it wrong only 92% of the time, as compared to 94% inaccuracy of the personality categories. The major qualms with the method is its lack of a sound scientific basis. The assertion that dichotomies exist seem to be vaccuous, as many studies have found a normal distribution over the categories, instead of the assumed bimodal distribution. In other words, people don&amp;rsquo;t really have much of a preference, and might be as introverted as they are extroverted.&lt;/p&gt;

&lt;p&gt;A far more acceptable scale chart is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Big_Five_personality_traits&#34; target=&#34;_blank&#34;&gt;Big Five personality taxonomy&lt;/a&gt;. Also known as the five-factor model (FFM), this theory relates usage of langauge to personality traits. For example, someone described as &lt;a href=&#34;https://en.wikipedia.org/wiki/Conscientious&#34; target=&#34;_blank&#34;&gt;conscientious&lt;/a&gt; is more likely to be described as &amp;ldquo;always prepared&amp;rdquo; rather than &amp;ldquo;messy&amp;rdquo;.  The five factors are: openness to experience, conscientiousness, extraversion, agreeableness and neuroticism, giving rise to acronym OCEAN. The causality behind these traits is attributed to both genetics and upbringing, roughly evenly. Some of these traits such as conscientiousness, extraversion, openness to experience, and neuroticism tend to be relatively stable from childhood through adulthood.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Wiki-grafik_peats-de_big_five_ENG.png/1232px-Wiki-grafik_peats-de_big_five_ENG.png&#34; data-caption=&#34;Big Five Personality Chart&#34;&gt;
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Wiki-grafik_peats-de_big_five_ENG.png/1232px-Wiki-grafik_peats-de_big_five_ENG.png&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Big Five Personality Chart
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Openness to experience here refers to the general appreciation for something new in one&amp;rsquo;s life, be it adventure, art or emotion. It can be associated with dynamic thinking, unpredictability (maybe not fickleness) and overall being explorative. Conversely, those who are less open-minded tend to play it &amp;ldquo;safe&amp;rdquo; and stick to perseverance and a bit of dogmatisim. Conscientiousness refers to how people control their behavioural tendencies: whether they are more of self-disciplinarians or free spirits, how much one prefers planning over spontaneity. Extraversion is similar to the MBTI regime, but recognises that individuals might be a combination of the two. Agreeableness reflects individual differences in general concern for social harmony. Agreeable individuals value getting along with others, and disagreeable individuals place self-interest above getting along with others. This more or less positively correlates with one&amp;rsquo;s social relationships and negatively with military or stern leadership. Finally, neuroticism is the tendency to experience negative emotions, such as anger, anxiety, or depression. Often termed as emotional instability, it tests our emotional reactiveness to stressful scenarios. The Big Five test has also been assessed on some animal species, such as &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2654334&#34; target=&#34;_blank&#34;&gt;chimpanzees&lt;/a&gt;, with concepts largely carrying forward.&lt;/p&gt;

&lt;p&gt;Overall, this module gave me a valuable insight into human behavioural tendencies, even if I had to take it with a pinch of salt in certain contexts. Certainly, being concious of such flaws and limitations in these methods helps us appreciate the diversity each one of us brings to the table.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reading Suggestions:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;As always, this is one of those topics where wikipedia provides a great introduction: &lt;a href=&#34;https://en.wikipedia.org/wiki/Myers–Briggs_Type_Indicator&#34; target=&#34;_blank&#34;&gt;Myers-Briggs Type Indicator&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;An online Big Five personality test: &lt;a href=&#34;https://www.truity.com/test/big-five-personality-test&#34; target=&#34;_blank&#34;&gt;Truity FFM&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A very balanced, textbook-ish approach to personality theories: &lt;a href=&#34;https://www.goodreads.com/book/show/1099831.The_Personality_Puzzle&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;The Personality Puzzle&lt;/em&gt; by David Funder&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ## Social Competence and Group Dynamics --&gt;

&lt;!-- ### Emotional Competence

**Keeping your cool**
**Role of healthy activities and sleep.**
**Emotional Insulation.** Allows for criticism to tolerable limits, such a way that it doesnot have dertrimental effects. Components and tools include ego  --&gt;
</description>
    </item>
    
    <item>
      <title>MRAS Bandits: From Control Theory to Slot Machines</title>
      <link>https://varun19299.github.io/research/mras/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/research/mras/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note: This work was done as a part of the course project under the guidance of &lt;a href=&#34;http://www.cse.iitm.ac.in/~prashla/&#34; target=&#34;_blank&#34;&gt;Prof. LA Prashanth&lt;/a&gt; for CS6700, IIT Madras. We are presently working on finite time analysis of this algorithm for both Regret Minimisation and Best Arm Identification settings.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-mras-algorithm&#34;&gt;The MRAS Algorithm&lt;/h3&gt;

&lt;h3 id=&#34;adapting-to-the-bandit-setting&#34;&gt;Adapting to the Bandit Setting&lt;/h3&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Yue &lt;em&gt;et al.&lt;/em&gt;, &lt;strong&gt;A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving&lt;/strong&gt;, CVPR 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla docs&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>References: Statement of Purpose</title>
      <link>https://varun19299.github.io/sop-references/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/sop-references/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, S Siddique, K Mitra, &lt;strong&gt;“Unified Image Recovery for Lensless Systems”&lt;/strong&gt;, IEEE International Conference on Computational Photography 2020 (Manuscript under Preparation).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;9th HULT Prize, Singapore&lt;/strong&gt;. Held at Nanyang Technology University (NTU), Singapore,16th to 18th March 2018.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;V Sundar&lt;/strong&gt;, A Sadhu, R Nevatia, &lt;strong&gt;“FARCNN: Decoupling attributes from object detection”&lt;/strong&gt;, Viterbi-IUSSTF (Indo US Science and Technology Forum) REU Symposium, July 2019.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/research/attribute-transfer&#34;&gt;Attribute Transfer Blog Post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/research/mras&#34;&gt;MRAS Bandits Blog Post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://varun19299.github.io/post/extend-lsd-slam&#34;&gt;Blog Post on extending LSD SLAM&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Commercial LiDARs using Physics based Game Engines</title>
      <link>https://varun19299.github.io/projects/lidar/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/projects/lidar/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Note: This work was done as a part of the &lt;strong&gt;MBRDI Hackathon 2019, Bangalore&lt;/strong&gt;. We placed third in the contest with our entry. For further details you can refer to our technical report or presentations.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;LiDAR point clouds are crucial in achieving level 4 and level 5 autonomous navigation. At the same time, obtaining LiDAR data for extensive scenes can be quite challenging. Hence, generating LiDAR data under synthetic simulations with control on parameters such as reflectivity, range, Field of View (FOV), etc would be useful. (See &lt;a href=&#34;https://www.youtube.com/watch?v=oZ7P4RsTE64&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; if you would like to see a how real LiDAR outputs look like.)&lt;/p&gt;

&lt;p&gt;In this project, we demonstrate an approach based on &lt;strong&gt;treating a camera as the equivalent of a LiDAR.&lt;/strong&gt; In fact, this approach has been used elsewhere in computational photography and photogrammetry (or geometric computer vision): a very similar concept is prevelant in the very construction of a Time of Flight camera.&lt;/p&gt;

&lt;p&gt;A careful reader at this point would interrupt us to ask a natural question. Why not consider ray-casting, which is offered by many physics and game engines alike? Why then should someone choose to resort to such a camera based approach? With all the existing work behind parallel compute for ray casting, it seems like the perfect candidate for the job. The picture at the top represents this, and is in fact from the simulation engine &lt;a href=&#34;http://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla&lt;/a&gt;. However, the team at &lt;strong&gt;MBRDI&lt;/strong&gt; wanted us to generate point clouds for a given road layout and for &lt;strong&gt;specified objects only&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;opendrive-layout&#34;&gt;OpenDRIVE Layout&lt;/h3&gt;

&lt;p&gt;The input data given is in the form of an &lt;a href=&#34;http://www.opendrive.org&#34; target=&#34;_blank&#34;&gt;OpenDRIVE&lt;/a&gt; layout, which essentially specifies road geometry and other objects (crossings, potential obstacles, poles, etc.). It &lt;strong&gt;does not&lt;/strong&gt; include information on road texture or renderings of objects present. In the below picture, we show a sample Opendrive file.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opendrive.png&#34; data-caption=&#34;Sample OpenDRIVE layout.&#34;&gt;
&lt;img src=&#34;opendrive.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;400&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample OpenDRIVE layout.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Note the same file rendered with software such as Sketchup 3D.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;sketch.png&#34; data-caption=&#34;Same OpenDRIVE layout rendered by Sketch3D.&#34;&gt;
&lt;img src=&#34;sketch.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Same OpenDRIVE layout rendered by Sketch3D.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;So, we wish to create a blackbox when given this road layout (and optionally some knowledge regarding objects on the road) can generate a 3D point cloud for specified targets, closely mimicking a real LiDAR.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;flow.png&#34; data-caption=&#34;Intended process flowchart.&#34;&gt;
&lt;img src=&#34;flow.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;400&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Intended process flowchart.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;image-lidar-calibration&#34;&gt;Image-LIDAR Calibration&lt;/h3&gt;

&lt;p&gt;Our first step is to draw correspondence between a given pixel and a 3D world point. This is done in a straightforward manner using projective geometry.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;proj.png&#34; data-caption=&#34;Proposed calibration scheme between a LiDAR and a camera. Credits: Yue et al., CVPR&amp;rsquo;18.&#34;&gt;
&lt;img src=&#34;proj.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Proposed calibration scheme between a LiDAR and a camera. Credits: Yue &lt;em&gt;et al.&lt;/em&gt;, CVPR&amp;rsquo;18.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;The goal of the calibration process is to find a function between each LiDAR Point and a pixel in the image. By utilising a depth map, we are able to do the converse too. The method we have adopted does the calibration automatically based on camera and LiDAR scanner parameters. It is similar to the camera perspective projection model once we set the camera and LiDAR at the same position, as shown in the above figure. Our method is based on work by Yue &lt;em&gt;et al.&lt;/em&gt;, CVPR 2018. We utilise their calibration scheme for our task as well.&lt;/p&gt;

&lt;p&gt;The problem is formulated as follows: for a certain laser ray  with azimuth angle &lt;code&gt;$\phi$&lt;/code&gt;  and zenith angle &lt;code&gt;$\theta$&lt;/code&gt;, calculate the  index &lt;code&gt;$(i,j)$&lt;/code&gt; of  the  corresponding  pixel  on  image. &lt;code&gt;${F_c},{F_o},{P}, {P^{\prime}}$&lt;/code&gt; and &lt;code&gt;$P_{far}$&lt;/code&gt; are  3-D  coordinates  of  a)  centre  of camera/LiDAR  scanner,  b)  centre  of  camera  near  clipping plane.  c)  point  first  hit  by  the  virtual  laser  ray  (in  red), d)  pixel  on  image  corresponding  to &lt;code&gt;$P$&lt;/code&gt;, e)  a  point  far away  in  the  laser  direction,  respectively. Also, &lt;code&gt;$m$&lt;/code&gt; and &lt;code&gt;$n$&lt;/code&gt; are  the width and height of the near clipping plane. &lt;code&gt;$\gamma$&lt;/code&gt; is &lt;code&gt;$1/2$&lt;/code&gt; vertical FOV of camera while &lt;code&gt;$\psi$&lt;/code&gt; is &lt;code&gt;$1/2$&lt;/code&gt; vertical FOV of the LiDAR scanner.  Note  that  LiDAR  scanner  FOV  is usually  smaller than camera FOV, since there is usually no object in the top part of the image, and the emitting laser to open space is not necessary.&lt;/p&gt;

&lt;p&gt;After a series of 3D geometry calculation, we can get:
&lt;code&gt;$$i = \frac{R_m}{m} (f\cdot tan\gamma \cdot \frac{m}{n} - \frac{f}{cos\theta} \cdot tan\phi)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$j = \frac{R_n}{n} (f\cdot tan\gamma + f \cdot tan\theta)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where &lt;code&gt;$f = \||\vec{F_c F_o}||$&lt;/code&gt;, and &lt;code&gt;$(R_m,R_n)$&lt;/code&gt; is the pixel resolution of the image/near clipping plane. Further, in order for the ray casting API to work properly, the 3D coordinates of &lt;code&gt;$P_{far}$&lt;/code&gt; are also required. Using similar 3D geometry calculations, we obtain:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$P^{\prime} = F_c + f \cdot \vec{x_c} - \frac{f}{cos\theta} \cdot tan \phi \cdot \vec{y_c} -f\cdot tan\theta \cdot \vec{z_c}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$P_{far}= F_c + k \cdot (P^{\prime}- F_c)$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where k is a large coefficient, and &lt;code&gt;$\vec{x_c}, \vec{y_c}, \vec{z_c}$&lt;/code&gt; are unit vectors of the camera axis in the world coordinate system. After simulation, both image and point cloud of the specified in-model scene are collected by the framework.&lt;/p&gt;

&lt;p&gt;With this step, we can associate each 3D point to a pixel and each (valid) pixel to a given 3D point according to the LiDAR&amp;rsquo;s construction.&lt;/p&gt;

&lt;h3 id=&#34;identify-targets-introduce-sparsity&#34;&gt;Identify Targets, Introduce Sparsity&lt;/h3&gt;

&lt;p&gt;The second step involves introducing artefacts such as sparsity and intensity variation typically found in real LiDARs such as a &lt;a href=&#34;http://velodynelidar.com/&#34; target=&#34;_blank&#34;&gt;Velodyne VLP-16&lt;/a&gt;. We can either do this by modelling the physics for reflectivity (with models of varying complexity) or by using data as a prior. Since our purpose here is to just demonstrate a proof of concept, we take the easier way out and turn to data-driven priors. In essence, we utilise a deep network which converts image taken by a camera to a sparse intensity map akin to a LiDAR.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;kitti.png&#34; data-caption=&#34;Sample sparse depth points from the KITTI Dataset.&#34;&gt;
&lt;img src=&#34;kitti.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample sparse depth points from the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/&#34; target=&#34;_blank&#34;&gt;KITTI Dataset&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;For the required data, we turn to the popular KITTI dataset, which includes VLP-16 acquisitions as shown below. For a visualisation of LiDAR data in the KITTI dataset, take a look at this excellant &lt;a href=&#34;https://navoshta.com/kitti-lidar/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;. A sample data point is shown in the figure above. In this step, we also choose to identify targets of interest according to the given road layout (which is in the OpenDrive format).&lt;/p&gt;

&lt;h3 id=&#34;reproject-points-back-to-3d&#34;&gt;Reproject Points back to 3D&lt;/h3&gt;

&lt;p&gt;Finally, having selected our objects of interest in the camera frame, we can reproject back to 3D. We select these objects using routine tools from computer vision such as segmentation, contour detection, object detection etc. In order to obtain intensities at those points, we simply follow a naive inverse-square power law model. Here&amp;rsquo;s a sample point cloud rendering.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;point_cloud.png&#34; data-caption=&#34;Sample point cloud.&#34;&gt;
&lt;img src=&#34;point_cloud.png&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;700&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample point cloud.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;concluding-notes&#34;&gt;Concluding Notes&lt;/h3&gt;

&lt;p&gt;Our solution to this hackathon conducted by &lt;strong&gt;MBRDI&lt;/strong&gt; involved a simple bijection  between LiDARs and cameras in the simulation world. With this correspondence, we are oepn to a lot of tools from the image processing and computer vision community. We&amp;rsquo;ve certainly been sub-optimal in a lot of places, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simple assumptions on intensity variation.&lt;/li&gt;
&lt;li&gt;Not handling multiple or specular reflections.&lt;/li&gt;
&lt;li&gt;Point clouds rendered is with respect to texture offered by such game engines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We shall try addressing some of these as future work.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Yue &lt;em&gt;et al.&lt;/em&gt;, &lt;strong&gt;A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving&lt;/strong&gt;, CVPR 2018.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://carla.readthedocs.io&#34; target=&#34;_blank&#34;&gt;Carla docs&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Education</title>
      <link>https://varun19299.github.io/education/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://varun19299.github.io/education/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s a bunch of courses I&amp;rsquo;ve taken as an undergrad at IIT-Madras:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;AI or Computer Vision Courses:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Computational Photography&lt;/li&gt;
&lt;li&gt;Advances in the theory of Deep Learning&lt;/li&gt;
&lt;li&gt;Multi-Armed Bandits&lt;/li&gt;
&lt;li&gt;Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Deep Learning for Computer Vision&lt;/li&gt;
&lt;li&gt;Simultaneous Localisation and Mapping (SLAM, &lt;em&gt;self study&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;GPU Programming (&lt;em&gt;audited&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Computer Vision (&lt;em&gt;audited&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;General Programming Courses:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Applied Programming Lab&lt;/li&gt;
&lt;li&gt;Data Structures and Algorithms&lt;/li&gt;
&lt;li&gt;Introduction to Programming&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Core Electrical Courses:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Analog circuits&lt;/li&gt;
&lt;li&gt;Analog devices&lt;/li&gt;
&lt;li&gt;Solid State Devices&lt;/li&gt;
&lt;li&gt;Microprocessors&lt;/li&gt;
&lt;li&gt;Control Theory&lt;/li&gt;
&lt;li&gt;Electrical Machines&lt;/li&gt;
&lt;li&gt;Signals and Systems; Discrete Signal Processing&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Math Courses:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Estimation Theory&lt;/li&gt;
&lt;li&gt;Convex Optimisation&lt;/li&gt;
&lt;li&gt;Complex Analysis&lt;/li&gt;
&lt;li&gt;Probability Theory&lt;/li&gt;
&lt;li&gt;Multivariable Calculus&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Other Science Electives:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Physics 1010, 1020 (Mechanics and Electromagnetics)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Humanities and Other Electives:&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Soft Skills&lt;/li&gt;
&lt;li&gt;German 1&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Grade transcript available on request.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
