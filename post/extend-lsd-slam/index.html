<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Varun Sundar">

  
  
  
    
  
  <meta name="description" content="Blog Post explaining the pipeline of Large Scale Direct (LSD) SLAM, its limitations and how recent advances in Computational Photography can help alleviate some of them. ">

  
  <link rel="alternate" hreflang="en-us" href="https://varun19299.github.io/post/extend-lsd-slam/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="https://varun19299.github.io/css/academic.min.613d5ab145b40da20c5c3a6f8b32e49a.css">

  

  




  


  

  <link rel="manifest" href="https://varun19299.github.io/index.webmanifest">
  <link rel="icon" type="image/png" href="https://varun19299.github.io/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="https://varun19299.github.io/img/icon-192.png">

  <link rel="canonical" href="https://varun19299.github.io/post/extend-lsd-slam/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Varun Sundar">
  <meta property="og:url" content="https://varun19299.github.io/post/extend-lsd-slam/">
  <meta property="og:title" content="Extending LSD SLAM with Computational Photography | Varun Sundar">
  <meta property="og:description" content="Blog Post explaining the pipeline of Large Scale Direct (LSD) SLAM, its limitations and how recent advances in Computational Photography can help alleviate some of them. "><meta property="og:image" content="https://varun19299.github.io/post/extend-lsd-slam/featured.png">
  <meta property="twitter:image" content="https://varun19299.github.io/post/extend-lsd-slam/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-11-28T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-12-11T12:51:35&#43;05:30">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://varun19299.github.io/post/extend-lsd-slam/"
  },
  "headline": "Extending LSD SLAM with Computational Photography",
  
  "image": [
    "https://varun19299.github.io/post/extend-lsd-slam/featured.png"
  ],
  
  "datePublished": "2019-11-28T00:00:00Z",
  "dateModified": "2019-12-11T12:51:35+05:30",
  
  "author": {
    "@type": "Person",
    "name": "V Sundar"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Varun Sundar",
    "logo": {
      "@type": "ImageObject",
      "url": "https://varun19299.github.io/img/icon-512.png"
    }
  },
  "description": "Blog Post explaining the pipeline of Large Scale Direct (LSD) SLAM, its limitations and how recent advances in Computational Photography can help alleviate some of them. "
}
</script>

  

  


  


  





  <title>Extending LSD SLAM with Computational Photography | Varun Sundar</title>

</head>


<body id="top" data-spy="scroll" data-offset="70"
  data-target="#TableOfContents"
  >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="https://varun19299.github.io/">Varun Sundar</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://varun19299.github.io/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="https://varun19299.github.io/education/"><span>Education</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://varun19299.github.io/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://varun19299.github.io/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="https://varun19299.github.io/contact/"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="https://varun19299.github.io/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  <script type="application/ld+json">
    {
        "@context" : "http://schema.org",
        "@type" : "BlogPosting",
        "mainEntityOfPage": {
             "@type": "WebPage",
             "@id": "https:\/\/varun19299.github.io\/"
        },
        "articleSection" : "post",
        "name" : "Extending LSD SLAM with Computational Photography",
        "headline" : "Extending LSD SLAM with Computational Photography",
        "description" : "Blog Post explaining the pipeline of Large Scale Direct (LSD) SLAM, its limitations and how recent advances in Computational Photography can help alleviate some of them.",
        "inLanguage" : "en-US",
        "author" : "",
        "creator" : "",
        "publisher": "",
        "accountablePerson" : "",
        "copyrightHolder" : "",
        "copyrightYear" : "2019",
        "datePublished": "2019-11-28 00:00:00 \x2b0000 UTC",
        "dateModified" : "2019-11-28 00:00:00 \x2b0000 UTC",
        "url" : "https:\/\/varun19299.github.io\/post\/extend-lsd-slam\/",
        "wordCount" : "2277",
        "keywords" : [ "Research Ideas","Blog" ]
    }
    </script>




























<div class="article-container pt-3">
  <h1>Extending LSD SLAM with Computational Photography</h1>

  

  



<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="https://varun19299.github.io/authors/v-sundar/">V Sundar</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Dec 11, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="https://varun19299.github.io/post/extend-lsd-slam/#disqus_thread"></a>
  

  
  

  
    

  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4"
  style="max-width: 720px; max-height: 456px;">
  <div style="position: relative">
    <img src="https://varun19299.github.io/post/extend-lsd-slam/featured_hufb673815033abd873a5a931b2f33f7b7_602567_720x0_resize_lanczos_2.png" alt="" class="featured-image">
    <span
      class="article-header-caption">Credits: Engel et al., ECCV 2014.</span>
  </div>
</div>


  <div class="article-container">

    <div class="article-style">
      

<p>In this post, we shall explore the key steps of <strong>Large Scale Direct SLAM</strong> (LSD SLAM) and understand its main strengths and weaknesses. We shall also see how we can possibly overcome some of these limitations with recent advances in Computational Photography.</p>

<h2 id="what-is-lsd-slam">What is LSD SLAM?</h2>

<p>LSD SLAM stands for <strong>Large Scale Direct SLAM</strong>, implying that it performs simultaneous localisation and mapping <em>directly</em> on the entire set of image intensities, instead of relying on a subset of points such as <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform" target="_blank">SIFT</a>, <a href="https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test" target="_blank">FAST</a> or <a href="https://en.wikipedia.org/wiki/Oriented_FAST_and_rotated_BRIEF" target="_blank">ORB</a> features. Essentially, this allows LSD SLAM to utilise all the information available in the image and thereby not discard crucial information contained in edges, which constitutes a major portion of the scene being mapped.</p>

<p>Where LSD SLAM crucially differs from earlier direct SLAM methods \cite, is the ingenious methods adopted for tracking and depth estimation. Depth estimation is done via <strong>small baseline</strong> stereo which allows for much fewer outliers. Tracking is done using direct image alignment over a subset of frames. These modifications allow LSD SLAM to be run real-time even with just a CPU. In comparison, previous approaches such as \cite required a complete high-end GPU to run real-time. <strong>The main novelty in this work include:</strong></p>

<ul>
<li><p>utilising key-frames to simplify monocular SLAM. Semi-dense depth maps are propogated only for key-frames and map optimisation is done only for their poses. <strong><em>This reduces the complexity of the bundle adjustment problem by a large margin.</em></strong></p></li>

<li><p>utilising a scale-aware image alignment algorithm to directly estimate the similarity transform <code>$\xi ∈ \mathcal{se}(3)$</code> between two keyframes. <strong><em>By doing so, we aren&rsquo;t restricted to only points pertaining to hand-engineered features.</em></strong></p></li>

<li><p>Incorporation of uncertainty of the estimated depth into tracking. For every point, before solving the correspondence problem, we roughly estimate whether it is worthwhile to do so, ie., <strong><em>is the computational cost worth the gain in depth accuracy?</em></strong></p></li>
</ul>

<p>Here, <code>$\mathcal{se}-3$</code> is the lie algebra of the Special Euclidean Objects group (SE-3). A lie algebra offers easier optimisation of variables by linearising the manifold of consideration. Intuitively, if we consider just rotation as a matrix, then it must be constrained to be orthogonal <code>($U^TU=I$)</code>. However, if we were optimising it with, say, gradient descent, then there is no guarantee that the resulting matrix would be orthogonal. Further, we only want to optimise on a subset of <code>$\mathbb{R}^{4\times 4}$</code>. Lie algebras encapsulate this effectively: the resultant <code>$\mathcal{se}-3 \in \mathbb{R}^6$</code> can be optimised by gradient descent. For more details, we refer you to (2).</p>

<h2 id="main-pipeline">Main Pipeline</h2>














<figure>


  <a data-fancybox="" href="pipeline.png" data-caption="Fig 1: Overview of LSD SLAM.">
<img src="pipeline.png" alt="" width="1400" height="1200" ></a>


  
  
  <figcaption>
    Fig 1: Overview of LSD SLAM.
  </figcaption>


</figure>


<p><strong>The pipeline follows:</strong></p>

<ol>
<li>Compute incoming pose via direct image alignment</li>
<li>Decide whether to initialise a key-frame</li>
<li>Compute depth via small baseline stereo</li>
<li>Repeat, finally perform map optimisation to correct for scale ambiguity</li>
</ol>

<p>We shall elaborate on each of these main steps in the sections below.</p>

<h3 id="concept-of-keyframes">Concept of Keyframes</h3>

<p>As mentioned before, LSD SLAM heavily utilises the concept of keyframes: &ldquo;special&rdquo; frames, which are initialised when a new frame has a pose significantly different from the current keyframe. The very first frame is taken to be a key-frame, with random normal depth, and random normal depth uncertainity. Map optimisation is performed only on the set of key-frames, further lowering computational costs associated with bundle adjustment and loop closure.</p>

<p>Each key-frame $K_j$ consists of an <strong>image frame, depth map, uncertainity map and pose vector</strong> - denoted by the tuple <code>$(I_j, D_j, U_j, p_j)$</code>, where <code>$I_j \in \mathbb{R}^{m\times n \times 3}$</code>, <code>$D_j \in \mathbb{R^+}^{m\times n}$</code>, <code>$U_j \in \mathbb{R^+}^{m\times n}$</code> and <code>$p_j \in \mathbb{R}^6$</code>. For non-key frames, we are not concerned with depth or uncertainity, and each non-key frame <code>$NK_j$</code> consists of the tuple <code>$(I_j, p_j)$</code>.</p>

<h3 id="direct-image-alignment">Direct Image Alignment</h3>

<p>The objective of direct image aligment is to use a key-frame  <code>$K_i$</code> and a new-frame <code>$I_j$</code> to determine <code>$p_j$</code>. This is done by a warping, where we project points from one image using the available depth information. If we know the pose of the second image, then we can recover the second image from these projected points. Thus, by optimising a function of the form below, we can find the pose of the second image.</p>

<p><code>$$\begin{align} p_j = \text{argmin}_p f(I_i - I_j[warp(D_i, p)]) \end{align} $$</code></p>

<p>Commonly used <code>$f$</code>, also called residue function, include <code>$f(x - y) = ||x-y||^2$</code> (MSE), or the hinge function (which ignores outliers), or L1 distance etc.</p>














<figure>


  <a data-fancybox="" href="warping.gif" data-caption="Fig 2: Warping images from the TUM room sequence.">
<img src="warping.gif" alt="" width="1200" height="750" ></a>


  
  
  <figcaption>
    Fig 2: Warping images from the <a href="https://vision.in.tum.de/data/datasets/rgbd-dataset/download" target="_blank">TUM room sequence</a>.
  </figcaption>


</figure>


<p>In the above figure, we demonstrate inverse image warping. These images have been taken from the <a href="https://vision.in.tum.de/data/datasets/rgbd-dataset/download" target="_blank">TUM room sequence</a>, a common SLAM benchmarking dataset. Black regions correspond to points where the Kinect failed to output any depth. Except when the target pose matches the source pose, this results in zero intensity regions. Providing a more complete depth map can offset this.</p>

<p>Since warping is so sensitive to depth information, it is important we optimise this function or residue only over points with lower variance in depth. This is where we can incorporate the uncertainity matrix <code>$U_i$</code>. One way of doing this is to divide the residue function element-wise by the uncertainity, yielding:</p>

<p><code>$$\begin{align} p_j = \text{argmin}_p f(\frac{I_i - I_j[warp(D_i, p)]}{g(U_i)}) \end{align} $$</code></p>

<p>The warp function, <code>$w(D, p):$ pixel $\to$ pixel</code>, takes a depth map <code>$D$</code> and a pixel point <code>$p$</code> and computes the corresponding pixel point in frame <code>$j$</code>. This requires projection to 3D world coordinates and re-projection to the second camera frame. Both of these require knowledge of the <a href="http://ftp.cs.toronto.edu/pub/psala/VM/camera-parameters.pdf" target="_blank">camera parameters</a>, which we implicitly assume to be known here. Written as a minimisation objective (sum over all pixels <code>$q \in \Sigma_D \subset [m]\times[n]$</code> in the image), this becomes:</p>

<p><code>$$\begin{align} p_j = \text{argmin}_p \sum_{q \in \Sigma_D \subset [m]\times[n]} f(\frac{I_i[q] - I_j[warp(D_i[q], p)]}{g(U_i[q])}) \end{align} $$</code></p>

<p>Giving us a similar objective as in Figure 1. LSD SLAM adopts <code>$g(U_i[q])= 2\sigma^2 + \frac{df(I_i[q] - I_j[warp(D_i[q], p)])}{dD_i[q]}^2U_i[q]$</code>. Intuitively, this penalises regions with continuous image intensity variation lesser than those with abrupt variations. We minimise this objective only over a subset of points for which we know depth information, hence being <strong>semi-dense image alignment</strong>.</p>

<p>Also note that we have not utilised quite a bit of prior information here. For instance, in the above picture, if you knew two corners of the monitor, you know where the other lie, since the monitor is rectangular. Using such information (priors) can greatly help improve the optimisation landscape. We shall discuss more about incorporating such data-driven priors in the limitations section.</p>

<p>Finally, we are optimising over the pose vector <code>$p_j$</code>. As a matrix, which is commonly used for such projective functions, the pose matrix has 16 entries. However, due to scale and rigid body constraints, this works out to just 6 degrees of freedom (dof). It is important to optimise the function over a linear landscape (or manifold), since it makes it much easier to converge towards an optimal solution. Here&rsquo;s where the <strong>lie algebra</strong> comes into play: it represents a linear manifold of the lie group, which is our pose matrix here. You can employ familiar first and second order optimisers including gradient descent, newton&rsquo;s method etc., but the most common is <a href="https://en.wikipedia.org/wiki/Gauss–Newton_algorithm" target="_blank">Gauss-Newton</a> optimisation.</p>

<h3 id="choosing-key-frames">Choosing Key-frames</h3>

<p>The motivation for choosing key-frames follows a very simple observation - LSD SLAM utilises small-baseline stereo matching, and hence it would be desirable if all the frames belonging to a particular key-frame group (all frames <code>$NK_j$</code> between <code>$K_i, K_{i+1}$</code>) are &ldquo;close&rdquo; to the key-frame <code>$K_i$</code>.</p>

<p>Hence, initialise a new key-frame if,</p>

<p><code>$$||p_j - p_i||&gt; \rho $$</code></p>

<p>When you initialise a new key-frame, use the projected points from the previous key-frames to initialise its depth map. The uncertainity map is similarly projected. Note that this might be semi-dense since not all projected points would cover the new key-frame. Here, to incorporate scale-drift tracking, LSD SLAM normalises the mean inverse depth to be 1. Scale awareness is incorporated during the final map-optimisation across various key-frames.</p>

<h3 id="small-baseline-stereo-matching">Small baseline stereo matching</h3>

<p>We perform many small baseline stereo-matching operations to refine the depth map of the key-frame <code>$K_i$</code>. We do not compute depth maps for non-key frames. In a similar manner to (2), only refinements where stereo depth is of higher confidence based on the accuracy of disparity search. This essentially allows us to propogate a semi-dense depth map throughout the whole SLAM pipeline, aiding computational simplicity. Such regions are often those with rich texture, which is expected in stereo based depth estimation.</p>














<figure>


  <a data-fancybox="" href="lsd_depth.jpeg" data-caption="Fig 3: Small-baseline stereo matching for depth refinement. (Left) Input frame, (Right) Semi-dense depth estimates.">
<img src="lsd_depth.jpeg" alt="" width="900" height="400" ></a>


  
  
  <figcaption>
    Fig 3: Small-baseline stereo matching for depth refinement. (<em>Left</em>) Input frame, (<em>Right</em>) Semi-dense depth estimates.
  </figcaption>


</figure>


<h3 id="map-optimisation">Map Optimisation</h3>

<p>At the end of tracking, we obtain a set of key-frames and their coarse pose-estimates. We are left with two pending tasks: (a) <strong>Incorporate global scale into these coarse pose-estimates</strong>. Remember, we initialised the depth map of a new key-frame with mean inverse depth as 1. (b) <strong>Further refine poses by minimising overall entropy</strong>.</p>

<p>We achieve the first by doing a similar photometric residual minimisation, but here over <code>$\xi ∈ \mathcal{sim}(3)$</code>, which has 7-degrees of freedom, the additional dof representing scale. This allows us to find scale parameters which best satisfy other key-frame constraints. This is written as:</p>

<p><code>$$\begin{align} p_j = \text{argmin}_p \sum_{q \in \Sigma_D \subset [m]\times[n]} f(\frac{I_i[q] - I_j[warp(D_i[q], p)]}{g_1(U_i[q])} + \frac{D_i[q] - D_j[warp(D_i[q], p)]}{g_2(U_i[q])}) \end{align} $$</code></p>

<p>Here, <code>$g_2$</code> is another smoothening term with respect to <code>$D_i[q] - D_j[warp(K_i[q], p)]$</code>. Again observe that map-optimisation is performed only among key-frames, hence we have access to <code>$D_j$</code>. In practice, <code>$\mathcal{sim}(3)$</code> tracking is computationally similar to <code>$\mathcal{se}(3)$</code> tracking.</p>

<h2 id="limitations-of-lsd-slam">Limitations of LSD SLAM</h2>

<p>While LSD SLAM certainly does produce quite impressive results, it does have its fair share of limitations too. We discuss the fundamental challenges behind a few of these below. Most of these limitations arise due to the restricted information available in a monocular video stream. And this is precisely where Computational Photography can be leveraged. By further considering information from physical camera models and their corresponding optics, we can alleviate a few of these challenges.</p>














<figure>


  <a data-fancybox="" href="lsd_slam_run.gif" data-caption="Fig 4: LSD SLAM results.">
<img src="lsd_slam_run.gif" alt="" width="500" height="500" ></a>


  
  
  <figcaption>
    Fig 4: LSD SLAM results.
  </figcaption>


</figure>


<h3 id="scale-ambiguity">Scale Ambiguity</h3>

<p>Monocular depth estimation techniques tends to lack a sense of absolute global scale. Thus, although our relative pose estimations within each key-frame may be accurate, this is no guarantee regarding the global map. While the map-optimisation procedure aims to mitigate this ambiguity, it is worthwhile noting that the <code>$\mathcal{sim}(3)$</code> objective is non-convex and hence depends to a major extent on the initialisation parameters. This step limits the convergence radius, which is how far LSD SLAM works before producing an inconsistent map.</p>

<p>A later paper from TU Munich - &ldquo;CNN SLAM&rdquo;, CVPR 2017 (4) - utilises depth predictions from CNNs for initialising key-frames. In theory, if the CNN is able to produce depths corresponding to the actual absolute depth, there would be no need for <code>$\mathcal{sim}(3)$</code> map optimisation. Unfortunately most state-of-the-art monocular depth estimation networks require a scaling factor to perform reliably on novel scenes. Hence, outside standard benchmarking sequences such as the TUM set or the <a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion" target="_blank">KITTI dataset</a>, absolute depth cannot be obtained. In the video below, CNN SLAM is run on an indoor room sequence.</p>


<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/z_NJxbkQnBU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>


<p>Instead of relying on triangulation based methods for obtaining depth, we can instead utilise depth from defocus. Again, for a monocular method, <strong>depth from defocus</strong> (DfD) requires a comparison image. However, utilising a technique from computational photography called <strong>coded aperture</strong>, we can obtain absolute depth using just a single image. The idea in coded aperture is similar to depth from defocus, where points out of focus are blurred by the Point Spread Function (PSF) associated with the lens. Unlike classical DfD, we have prior knowledge regarding these Point Spread Functions and can hence determine depth from a single image itself.</p>

<p>One recent method in coded aperture (5), utilises optimised phase masks to determine these PSFs, producing much more reliable depth maps. It also doesn&rsquo;t suffer from a previous well-known limitation of such techniques: loss of illumination intensity.</p>














<figure>


  <a data-fancybox="" href="phasecam.png" data-caption="Fig 5: Sample depth maps from PhaseCam3D (5). Compared against a Kinect. Notice how accurate the absolute groundt ruth outputs are.">
<img src="phasecam.png" alt="" width="800" height="1200" ></a>


  
  
  <figcaption>
    Fig 5: Sample depth maps from PhaseCam3D (5). Compared against a Kinect. Notice how accurate the absolute groundt ruth outputs are.
  </figcaption>


</figure>


<p>For more details regarding the formation of such PSFs, which involves <a href="https://en.wikipedia.org/wiki/Fresnel_diffraction" target="_blank">Fresnel Propogation</a>, we would refer you to (5).</p>

<h3 id="pure-rotation-settings">Pure Rotation Settings</h3>

<p>Another scenario where LSD SLAM fails to track frames is under pure rotation. Small scale stereo matching fails in this setting due to the lack of a baseline. Further, since LSD SLAM relies on visual odometry (and not any other inputs from sensors etc.), once tracking fails, it cannot recover by re-localisation or track through strong rotations by using previously triangulated geometry.</p>

<p>CNN SLAM, however, does work under this scenario, provided that the CNN can generate relatively consistent depth maps for the encountered frames. Again, this does not necessarily imply that the absolute depths would be consistent, especially for novel scenes where most present monocular methods have trouble generalising.</p>














<figure>


  <a data-fancybox="" href="cnn_slam.png" data-caption="Fig 6: Comparison on a sequence that includes mostly pure rotational camera motion between the reconstruction obtained by groundtruth depth (left), CNN-SLAM (middle) and LSD-SLAM (right). Source: CNN SLAM (4), figure 5.">
<img src="cnn_slam.png" alt="" width="600" height="900" ></a>


  
  
  <figcaption>
    Fig 6: Comparison on a sequence that includes mostly pure rotational camera motion between the reconstruction obtained by <strong>groundtruth</strong> depth <em>(left)</em>, <strong>CNN-SLAM</strong> <em>(middle)</em> and <strong>LSD-SLAM</strong> <em>(right)</em>. Source: CNN SLAM (4), figure 5.
  </figcaption>


</figure>


<h2 id="other-extensions">Other Extensions</h2>

<h3 id="using-polarisation-for-texture-less-regions">Using polarisation for texture-less regions</h3>

<h3 id="explicit-priors-for-pose-estimation">Explicit priors for pose-estimation</h3>

<p>Make pose-estimation a lot cheaper.</p>

<h3 id="can-we-better-represent-maps">Can we better represent maps?</h3>

<h2 id="references">References</h2>

<ol>
<li><p>Thomas Schöps, Jacob Engel and Daniel Cremers. <a href="https://vision.in.tum.de/research/vslam/lsdslam" target="_blank">&ldquo;LSD-SLAM: Large-Scale Direct Monocular SLAM&rdquo;</a>. In Proceedings of the European Conference on Computer Vision (ECCV), 2014.</p></li>

<li><p>Jose Blanco.<a href="http://ingmec.ual.es/~jlblanco/papers/jlblanco2010geometry3D_techrep.pdf" target="_blank">&ldquo;A tutorial on SE(3) transformation parameterizations and on-manifold optimization&rdquo;</a>. MAPIR Group.</p></li>

<li><p>Jacob Engel, Jurgen Sturm and Daniel Cremers. <a href="https://jsturm.de/publications/data/engel2013iccv.pdf" target="_blank">&ldquo;Semi-Dense Visual Odometry for a Monocular Camera&rdquo;</a>. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013.</p></li>

<li><p>Keisuke Tatento, Federico Tombari, Iro Laina and Nassir Navab. <a href="https://arxiv.org/abs/1704.03489" target="_blank">&ldquo;CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction&rdquo;</a>. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p></li>

<li><p>Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin C. Sankaranarayanan, and Ashok Veeraraghavan. <a href="http://imagesci.ece.cmu.edu/files/paper/2019/PhaseCam_ICCP19.pdf" target="_blank">&ldquo;Phasecam3d — Learning phase masks for passive single view depth estimation&rdquo;</a>. In IEEE Intl. Conf. Computational Photography (ICCP), 2019.</p></li>
</ol>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="https://varun19299.github.io/tags/research-ideas/">Research Ideas</a>
  
</div>



    
    








  
  
    
  
  






  
  
  
  
  <div class="media author-card">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://varun19299.github.io/authors/v-sundar/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>



    
    
    

    

    
<section id="comments">
  
    

  
</section>



  </div>
</article>

<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

      

    
    
    
    <script src="https://varun19299.github.io/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="https://varun19299.github.io/js/academic.min.0bf1e3db85cbb232372ed31d6f10dc70.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>

  

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\[','\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
           extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  });
  </script>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


</body>

</html>